{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistemas de recommendação - Neural Collaborative Filtering\n",
    "> Demo sobre Neural Collaborative Filtering\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [demo, neural networks, deep learning, recommender systems, paper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso esteja rodando o notebook não esqueça de fazer o download das bibliotecas e inicie o `tensorboard` no background:\n",
    "\n",
    "No terminal\n",
    "```bash\n",
    "pip install tensorflow lightfm pandas\n",
    "tensorboard --logdir 2020-09-11-neural_collaborative_filter/logs\n",
    "```\n",
    "\n",
    "ou no notebook\n",
    "```python\n",
    "!pip install tensorflow lightfm pandas\n",
    "```\n",
    "\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "!tensorboard --logdir 2020-09-11-neural_collaborative_filter/logs &\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/inteligencia-superficial/lib/python3.7/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import lightfm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_movielens\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from scipy import sparse\n",
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.3.0\n",
      "LightFM version: 1.15\n",
      "Pandas version: 1.1.1\n",
      "Numpy version: 1.18.5\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"LightFM version: {lightfm.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "TOP_K = 5\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Os dados\n",
    "\n",
    "#https://github.com/murilo-cunha/inteligencia-superficial/blob/master/images/avatar.png\n",
    "\n",
    "![](https://github.com/murilo-cunha/inteligencia-superficial/blob/master/images/2020-09-11-neural_collaborative_filter/matrix_factorization_with_alpha.png \"Credit: https://developers.google.com/machine-learning/recommendation/collaborative/basics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction matrix:\n",
      "[[5 3 4 3 3 5 4 0 5 3]\n",
      " [4 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [4 0 0 0 0 0 0 4 4 0]\n",
      " [0 0 0 5 0 0 5 5 5 4]\n",
      " [0 0 0 0 0 0 3 0 0 0]\n",
      " [0 0 0 0 0 0 4 0 0 0]\n",
      " [4 0 0 4 0 0 0 0 4 0]]\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "data = fetch_movielens(min_rating=3.0)\n",
    "\n",
    "print(\"Interaction matrix:\")\n",
    "print(data[\"train\"].toarray()[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction matrix:\n",
      "[[1 1 1 1 1 1 1 0 1 1]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 1 0 0 1 1 1 1]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 0]]\n",
      "\n",
      "Ratings:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "#collapse\n",
    "for dataset in [\"test\", \"train\"]:\n",
    "    data[dataset] = (data[dataset].toarray() > 0).astype(\"int8\")\n",
    "\n",
    "# Make the ratings binary\n",
    "print(\"Interaction matrix:\")\n",
    "print(data[\"train\"][:10, :10])\n",
    "\n",
    "print(\"\\nRatings:\")\n",
    "unique_ratings = np.unique(data[\"train\"])\n",
    "print(unique_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def wide_to_long(wide: np.array, possible_ratings: List[int]) -> np.array:\n",
    "    \"\"\"Go from wide table to long.\n",
    "    :param wide: wide array with user-item interactions\n",
    "    :param possible_ratings: list of possible ratings that we may have.\"\"\"\n",
    "\n",
    "    def _get_ratings(arr: np.array, rating: int) -> np.array:\n",
    "        \"\"\"Generate long array for the rating provided\n",
    "        :param arr: wide array with user-item interactions\n",
    "        :param rating: the rating that we are interested\"\"\"\n",
    "        idx = np.where(arr == rating)\n",
    "        return np.vstack(\n",
    "            (idx[0], idx[1], np.ones(idx[0].size, dtype=\"int8\") * rating)\n",
    "        ).T\n",
    "\n",
    "    long_arrays = []\n",
    "    for r in possible_ratings:\n",
    "        long_arrays.append(_get_ratings(wide, r))\n",
    "\n",
    "    return np.vstack(long_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_train = wide_to_long(data[\"train\"], unique_ratings)\n",
    "df_train = pd.DataFrame(long_train, columns=[\"user_id\", \"item_id\", \"interaction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All interactions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  interaction\n",
       "0        0        7            0\n",
       "1        0       10            0\n",
       "2        0       19            0\n",
       "3        0       20            0\n",
       "4        0       26            0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "print(\"All interactions:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only positive interactions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1511499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511500</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511501</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511502</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511503</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  interaction\n",
       "1511499        0        0            1\n",
       "1511500        0        1            1\n",
       "1511501        0        2            1\n",
       "1511502        0        3            1\n",
       "1511503        0        4            1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "print(\"Only positive interactions:\")\n",
    "df_train[df_train[\"interaction\"] > 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O modelo (Neural Collaborative Filtering)\n",
    "\n",
    "<center><img src=\"https://github.com/murilo-cunha/inteligencia-superficial/blob/master/images/2020-09-11-neural_collaborative_filter/ncf_all_with_alpha.png\" width=\"70%\" url=\"https://developers.google.com/machine-learning/recommendation/collaborative/basics\" description=\"Fonte: https://developers.google.com/machine-learning/recommendation/collaborative/basics\" /> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    Multiply,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_ncf(\n",
    "    number_of_users: int,\n",
    "    number_of_items: int,\n",
    "    latent_dim_mf: int = 4,\n",
    "    latent_dim_mlp: int = 32,\n",
    "    reg_mf: int = 0,\n",
    "    reg_mlp: int = 0.01,\n",
    "    dense_layers: List[int] = [8, 4],\n",
    "    reg_layers: List[int] = [0.01, 0.01],\n",
    "    activation_dense: str = \"relu\",\n",
    ") -> keras.Model:\n",
    "\n",
    "    # input layer\n",
    "    user = Input(shape=(), dtype=\"int32\", name=\"user_id\")\n",
    "    item = Input(shape=(), dtype=\"int32\", name=\"item_id\")\n",
    "\n",
    "    # embedding layers\n",
    "    mf_user_embedding = Embedding(\n",
    "        input_dim=number_of_users,\n",
    "        output_dim=latent_dim_mf,\n",
    "        name=\"mf_user_embedding\",\n",
    "        embeddings_initializer=\"RandomNormal\",\n",
    "        embeddings_regularizer=l2(reg_mf),\n",
    "        input_length=1,\n",
    "    )\n",
    "    mf_item_embedding = Embedding(\n",
    "        input_dim=number_of_items,\n",
    "        output_dim=latent_dim_mf,\n",
    "        name=\"mf_item_embedding\",\n",
    "        embeddings_initializer=\"RandomNormal\",\n",
    "        embeddings_regularizer=l2(reg_mf),\n",
    "        input_length=1,\n",
    "    )\n",
    "\n",
    "    mlp_user_embedding = Embedding(\n",
    "        input_dim=number_of_users,\n",
    "        output_dim=latent_dim_mlp,\n",
    "        name=\"mlp_user_embedding\",\n",
    "        embeddings_initializer=\"RandomNormal\",\n",
    "        embeddings_regularizer=l2(reg_mlp),\n",
    "        input_length=1,\n",
    "    )\n",
    "    mlp_item_embedding = Embedding(\n",
    "        input_dim=number_of_items,\n",
    "        output_dim=latent_dim_mlp,\n",
    "        name=\"mlp_item_embedding\",\n",
    "        embeddings_initializer=\"RandomNormal\",\n",
    "        embeddings_regularizer=l2(reg_mlp),\n",
    "        input_length=1,\n",
    "    )\n",
    "\n",
    "    # MF vector\n",
    "    mf_user_latent = Flatten()(mf_user_embedding(user))\n",
    "    mf_item_latent = Flatten()(mf_item_embedding(item))\n",
    "    mf_cat_latent = Multiply()([mf_user_latent, mf_item_latent])\n",
    "\n",
    "    # MLP vector\n",
    "    mlp_user_latent = Flatten()(mlp_user_embedding(user))\n",
    "    mlp_item_latent = Flatten()(mlp_item_embedding(item))\n",
    "    mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "\n",
    "    mlp_vector = mlp_cat_latent\n",
    "\n",
    "    # build dense layers for model\n",
    "    for i in range(len(dense_layers)):\n",
    "        layer = Dense(\n",
    "            dense_layers[i],\n",
    "            activity_regularizer=l2(reg_layers[i]),\n",
    "            activation=activation_dense,\n",
    "            name=\"layer%d\" % i,\n",
    "        )\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    predict_layer = Concatenate()([mf_cat_latent, mlp_vector])\n",
    "\n",
    "    result = Dense(\n",
    "        1, activation=\"sigmoid\", kernel_initializer=\"lecun_uniform\", name=\"interaction\"\n",
    "    )\n",
    "\n",
    "    output = result(predict_layer)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[user, item],\n",
    "        outputs=[output],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"neural_collaborative_filtering\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding (Embedding)  (None, 32)           30176       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mlp_item_embedding (Embedding)  (None, 32)           53824       item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32)           0           mlp_user_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           mlp_item_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "mf_user_embedding (Embedding)   (None, 4)            3772        user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mf_item_embedding (Embedding)   (None, 4)            6728        item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4)            0           mf_user_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4)            0           mf_item_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer0 (Dense)                  (None, 8)            520         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 4)            0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 4)            36          layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8)            0           multiply[0][0]                   \n",
      "                                                                 layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "interaction (Dense)             (None, 1)            9           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 95,065\n",
      "Trainable params: 95,065\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#collapse\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "n_users, n_items = data[\"train\"].shape\n",
    "ncf_model = create_ncf(n_users, n_items)\n",
    "\n",
    "ncf_model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "        tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "        tf.keras.metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")\n",
    "ncf_model._name = \"neural_collaborative_filtering\"\n",
    "ncf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    targets: List[str],\n",
    "    val_split: float = 0.1,\n",
    "    batch_size: int = 512,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"Make TensorFlow dataset from Pandas DataFrame.\n",
    "    :param df: input DataFrame - only contains features and target(s)\n",
    "    :param targets: list of columns names corresponding to targets\n",
    "    :param val_split: fraction of the data that should be used for validation\n",
    "    :param batch_size: batch size for training\n",
    "    :param seed: random seed for shuffling the data - setting to `None` will not shuffle the data\"\"\"\n",
    "\n",
    "    n_val = round(df.shape[0] * val_split)\n",
    "    if seed:\n",
    "        # shuffle all the rows\n",
    "        x = df.sample(frac=1, random_state=seed).to_dict(\"series\")\n",
    "    else:\n",
    "        x = df.to_dict(\"series\")\n",
    "    y = dict()\n",
    "    for t in targets:\n",
    "        y[t] = x.pop(t)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "    ds_val = ds.take(n_val).batch(batch_size)\n",
    "    ds_train = ds.skip(n_val).batch(batch_size)\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation datasets\n",
    "ds_train, ds_val = make_tf_dataset(df_train, [\"interaction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/inteligencia-superficial/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/2789 [..............................] - ETA: 0s - loss: 2.7761 - tp: 1.0000 - fp: 32.0000 - tn: 459.0000 - fn: 20.0000 - accuracy: 0.8984 - precision: 0.0303 - recall: 0.0476 - auc: 0.4325WARNING:tensorflow:From /usr/local/anaconda3/envs/inteligencia-superficial/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0254s). Check your callbacks.\n",
      "2789/2789 [==============================] - 9s 3ms/step - loss: 0.2318 - tp: 1691.0000 - fp: 789.0000 - tn: 1359625.0000 - fn: 65408.0000 - accuracy: 0.9536 - precision: 0.6819 - recall: 0.0252 - auc: 0.8033 - val_loss: 0.1408 - val_tp: 902.0000 - val_fp: 416.0000 - val_tn: 150669.0000 - val_fn: 6626.0000 - val_accuracy: 0.9556 - val_precision: 0.6844 - val_recall: 0.1198 - val_auc: 0.9020\n",
      "Epoch 2/10\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1279 - tp: 11668.0000 - fp: 6340.0000 - tn: 1354074.0000 - fn: 55431.0000 - accuracy: 0.9567 - precision: 0.6479 - recall: 0.1739 - auc: 0.9164 - val_loss: 0.1236 - val_tp: 1532.0000 - val_fp: 854.0000 - val_tn: 150231.0000 - val_fn: 5996.0000 - val_accuracy: 0.9568 - val_precision: 0.6421 - val_recall: 0.2035 - val_auc: 0.9195\n",
      "Epoch 3/10\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1191 - tp: 13715.0000 - fp: 7758.0000 - tn: 1352656.0000 - fn: 53384.0000 - accuracy: 0.9572 - precision: 0.6387 - recall: 0.2044 - auc: 0.9254 - val_loss: 0.1198 - val_tp: 1587.0000 - val_fp: 835.0000 - val_tn: 150250.0000 - val_fn: 5941.0000 - val_accuracy: 0.9573 - val_precision: 0.6552 - val_recall: 0.2108 - val_auc: 0.9232\n",
      "Epoch 4/10\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1148 - tp: 14333.0000 - fp: 7576.0000 - tn: 1352838.0000 - fn: 52766.0000 - accuracy: 0.9577 - precision: 0.6542 - recall: 0.2136 - auc: 0.9293 - val_loss: 0.1160 - val_tp: 1610.0000 - val_fp: 797.0000 - val_tn: 150288.0000 - val_fn: 5918.0000 - val_accuracy: 0.9577 - val_precision: 0.6689 - val_recall: 0.2139 - val_auc: 0.9267\n",
      "Epoch 5/10\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1114 - tp: 15531.0000 - fp: 7649.0000 - tn: 1352765.0000 - fn: 51568.0000 - accuracy: 0.9585 - precision: 0.6700 - recall: 0.2315 - auc: 0.9335 - val_loss: 0.1138 - val_tp: 1777.0000 - val_fp: 877.0000 - val_tn: 150208.0000 - val_fn: 5751.0000 - val_accuracy: 0.9582 - val_precision: 0.6696 - val_recall: 0.2361 - val_auc: 0.9294\n",
      "Epoch 6/10\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1088 - tp: 16978.0000 - fp: 8344.0000 - tn: 1352070.0000 - fn: 50121.0000 - accuracy: 0.9590 - precision: 0.6705 - recall: 0.2530 - auc: 0.9373 - val_loss: 0.1120 - val_tp: 1927.0000 - val_fp: 975.0000 - val_tn: 150110.0000 - val_fn: 5601.0000 - val_accuracy: 0.9585 - val_precision: 0.6640 - val_recall: 0.2560 - val_auc: 0.9317\n",
      "Epoch 7/10\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1069 - tp: 18235.0000 - fp: 9057.0000 - tn: 1351357.0000 - fn: 48864.0000 - accuracy: 0.9594 - precision: 0.6681 - recall: 0.2718 - auc: 0.9401 - val_loss: 0.1108 - val_tp: 2033.0000 - val_fp: 1031.0000 - val_tn: 150054.0000 - val_fn: 5495.0000 - val_accuracy: 0.9589 - val_precision: 0.6635 - val_recall: 0.2701 - val_auc: 0.9338\n",
      "Epoch 8/10\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1055 - tp: 19127.0000 - fp: 9621.0000 - tn: 1350793.0000 - fn: 47972.0000 - accuracy: 0.9597 - precision: 0.6653 - recall: 0.2851 - auc: 0.9421 - val_loss: 0.1100 - val_tp: 2113.0000 - val_fp: 1069.0000 - val_tn: 150016.0000 - val_fn: 5415.0000 - val_accuracy: 0.9591 - val_precision: 0.6640 - val_recall: 0.2807 - val_auc: 0.9350\n",
      "Epoch 9/10\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1046 - tp: 19749.0000 - fp: 9984.0000 - tn: 1350430.0000 - fn: 47350.0000 - accuracy: 0.9598 - precision: 0.6642 - recall: 0.2943 - auc: 0.9435 - val_loss: 0.1094 - val_tp: 2154.0000 - val_fp: 1107.0000 - val_tn: 149978.0000 - val_fn: 5374.0000 - val_accuracy: 0.9591 - val_precision: 0.6605 - val_recall: 0.2861 - val_auc: 0.9357\n",
      "Epoch 10/10\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1040 - tp: 20082.0000 - fp: 10168.0000 - tn: 1350246.0000 - fn: 47017.0000 - accuracy: 0.9599 - precision: 0.6639 - recall: 0.2993 - auc: 0.9445 - val_loss: 0.1090 - val_tp: 2191.0000 - val_fp: 1126.0000 - val_tn: 149959.0000 - val_fn: 5337.0000 - val_accuracy: 0.9593 - val_precision: 0.6605 - val_recall: 0.2910 - val_auc: 0.9364\n",
      "CPU times: user 2min 29s, sys: 43.4 s, total: 3min 12s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define logs and callbacks\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=0\n",
    ")\n",
    "\n",
    "train_hist = ncf_model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_test = wide_to_long(data[\"train\"], unique_ratings)\n",
    "df_test = pd.DataFrame(long_test, columns=[\"user_id\", \"item_id\", \"interaction\"])\n",
    "ds_test, _ = make_tf_dataset(df_test, [\"interaction\"], val_split=0, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.81 s, sys: 210 ms, total: 4.02 s\n",
      "Wall time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ncf_predictions = ncf_model.predict(ds_test)\n",
    "df_test[\"ncf_predictions\"] = ncf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>interaction</th>\n",
       "      <th>ncf_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.523643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.123813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  interaction  ncf_predictions\n",
       "0        0        7            0         0.523643\n",
       "1        0       10            0         0.719504\n",
       "2        0       19            0         0.100669\n",
       "3        0       20            0         0.123813\n",
       "4        0       26            0         0.102480"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# sanity checks - stop execution if we have low standard deviation (all recommendations are the same)\n",
    "std = df_test.describe().loc[\"std\", \"ncf_predictions\"]\n",
    "if std < 0.01:\n",
    "    raise ValueError(\"Model predictions have standard deviation of less than 1e-2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural collaborative filtering predictions\n",
      "[[7.7809501e-01 3.4897393e-01 2.3736593e-01 7.5093412e-01]\n",
      " [1.5352371e-01 1.8476248e-03 2.3163706e-02 3.6399364e-03]\n",
      " [4.6624422e-02 4.7096610e-04 1.2840241e-02 1.1576419e-04]\n",
      " [8.5962385e-02 1.4925003e-03 6.1967373e-03 5.1632524e-04]\n",
      " [5.8516884e-01 2.8336483e-01 7.5634271e-02 3.0715367e-01]\n",
      " [4.0988737e-01 2.2669524e-02 1.0599941e-02 4.0282601e-01]\n",
      " [6.0177052e-01 6.6075641e-01 7.8367621e-02 8.1673837e-01]\n",
      " [4.9012059e-01 8.9323461e-02 6.3689947e-03 6.7939401e-02]\n",
      " [1.5069479e-01 1.3713539e-03 2.8979778e-04 2.2239387e-03]\n",
      " [5.0181168e-01 6.9155514e-02 3.4887791e-02 4.8452517e-01]]\n"
     ]
    }
   ],
   "source": [
    "#collapse\n",
    "data[\"ncf_predictions\"] = df_test.pivot(\n",
    "    index=\"user_id\", columns=\"item_id\", values=\"ncf_predictions\"\n",
    ").values\n",
    "print(\"Neural collaborative filtering predictions\")\n",
    "print(data[\"ncf_predictions\"][:10, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At K = 5, we have a precision of 0.10859 and a recall of 0.06487\n"
     ]
    }
   ],
   "source": [
    "precision_ncf = tf.keras.metrics.Precision(top_k=TOP_K)\n",
    "recall_ncf = tf.keras.metrics.Recall(top_k=TOP_K)\n",
    "\n",
    "precision_ncf.update_state(data[\"test\"], data[\"ncf_predictions\"])\n",
    "recall_ncf.update_state(data[\"test\"], data[\"ncf_predictions\"])\n",
    "print(\n",
    "    f\"At K = {TOP_K}, we have a precision of {precision_ncf.result().numpy():.5f} and a recall of {recall_ncf.result().numpy():.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At K = 5, we have a precision of 0.10541 and a recall of 0.06297\n",
      "CPU times: user 1.01 s, sys: 235 ms, total: 1.25 s\n",
      "Wall time: 858 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LightFM model\n",
    "norm = lambda x: (x - np.min(x)) / np.ptp(x)\n",
    "lightfm_model = LightFM(loss=\"warp\")\n",
    "lightfm_model.fit(sparse.coo_matrix(data[\"train\"]), epochs=N_EPOCHS)\n",
    "\n",
    "lightfm_predictions = lightfm_model.predict(\n",
    "    df_test[\"user_id\"].values, df_test[\"item_id\"].values\n",
    ")\n",
    "df_test[\"lightfm_predictions\"] = lightfm_predictions\n",
    "wide_predictions = df_test.pivot(\n",
    "    index=\"user_id\", columns=\"item_id\", values=\"lightfm_predictions\"\n",
    ").values\n",
    "data[\"lightfm_predictions\"] = norm(wide_predictions)\n",
    "\n",
    "# compute the metrics\n",
    "precision_lightfm = tf.keras.metrics.Precision(top_k=TOP_K)\n",
    "recall_lightfm = tf.keras.metrics.Recall(top_k=TOP_K)\n",
    "precision_lightfm.update_state(data[\"test\"], data[\"lightfm_predictions\"])\n",
    "recall_lightfm.update_state(data[\"test\"], data[\"lightfm_predictions\"])\n",
    "print(\n",
    "    f\"At K = {TOP_K}, we have a precision of {precision_lightfm.result().numpy():.5f} and a recall of {recall_lightfm.result().numpy():.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
