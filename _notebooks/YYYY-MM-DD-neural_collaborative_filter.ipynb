{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install tensorflow lightfm pandas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/inteligencia-superficial/lib/python3.7/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import lightfm\n",
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_movielens\n",
    "from lightfm.evaluation import precision_at_k\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.3.0\n",
      "LightFM version: 1.15\n",
      "Pandas version: 1.1.1\n",
      "Numpy version: 1.18.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"LightFM version: {lightfm.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction matrix:\n",
      "[[5 3 4 3 3 5 4 0 5 3]\n",
      " [4 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [4 0 0 0 0 0 0 4 4 0]\n",
      " [0 0 0 5 0 0 5 5 5 4]\n",
      " [0 0 0 0 0 0 3 0 0 0]\n",
      " [0 0 0 0 0 0 4 0 0 0]\n",
      " [4 0 0 4 0 0 0 0 4 0]]\n"
     ]
    }
   ],
   "source": [
    "data = fetch_movielens(min_rating=3.0)\n",
    "\n",
    "print(\"Interaction matrix:\")\n",
    "print(data['train'].toarray()[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction matrix:\n",
      "[[1 1 1 1 1 1 1 0 1 1]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 1 0 0 1 1 1 1]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 0]]\n",
      "\n",
      "Ratings:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['test', 'train']:\n",
    "    data[dataset] = (data[dataset].toarray() > 0).astype('int8')\n",
    "    \n",
    "# Make the ratings binary\n",
    "print(\"Interaction matrix:\")\n",
    "print(data['train'][:10,:10])\n",
    "\n",
    "print(\"\\nRatings:\")\n",
    "unique_ratings = np.unique(data['train'])\n",
    "print(unique_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def wide_to_long(wide: np.array, possible_ratings: List) -> np.array:\n",
    "    \n",
    "    def _get_ratings(arr: np.array, rating: int) -> np.array:\n",
    "        idx = np.where(arr == rating)\n",
    "        return np.vstack((idx[0],idx[1], np.ones(idx[0].size, dtype='int8') * rating)).T\n",
    "    \n",
    "    long_arrays = []\n",
    "    for r in possible_ratings:\n",
    "        long_arrays.append(_get_ratings(wide, r))\n",
    "    \n",
    "    return np.vstack(long_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_train = wide_to_long(data['train'], unique_ratings)\n",
    "df_train = pd.DataFrame(long_train, columns = ['user_id', 'item_id', 'interaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All interactions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  interaction\n",
       "0        0        7            0\n",
       "1        0       10            0\n",
       "2        0       19            0\n",
       "3        0       20            0\n",
       "4        0       26            0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"All interactions:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Only positive interactions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1511499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511500</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511501</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511502</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511503</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  interaction\n",
       "1511499        0        0            1\n",
       "1511500        0        1            1\n",
       "1511501        0        2            1\n",
       "1511502        0        3            1\n",
       "1511503        0        4            1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nOnly positive interactions:\")\n",
    "df_train[df_train['interaction'] > 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    Input,\n",
    "    Dense,\n",
    "    Multiply,\n",
    "    Flatten,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_ncf(\n",
    "    number_of_users: int,\n",
    "    number_of_items: int,\n",
    "    latent_dim_mf: int = 4,\n",
    "    latent_dim_mlp: int = 16,\n",
    "    reg_mf: int = 0,\n",
    "    reg_mlp: int = 0.01,\n",
    "    dense_layers: List[int] = [8, 4], # try different things here - maybe try the architecture from the paper?\n",
    "    reg_layers: List[int] = [0.1, 0.1], # best results was without the NN part really\n",
    "    activation_dense: str = \"relu\"\n",
    ") -> keras.Model:\n",
    "\n",
    "    # input layer\n",
    "    user = Input(shape=(), dtype=\"int32\", name='user_id')\n",
    "    item = Input(shape=(), dtype=\"int32\", name='item_id')\n",
    "\n",
    "    # embedding layers\n",
    "    mf_user_embedding = Embedding(\n",
    "        input_dim=number_of_users,\n",
    "        output_dim=latent_dim_mf,\n",
    "        name=\"mf_user_embedding\",\n",
    "        embeddings_initializer=\"RandomNormal\",\n",
    "        embeddings_regularizer=l2(reg_mf),\n",
    "        input_length=1,\n",
    "    )\n",
    "    mf_item_embedding = Embedding(\n",
    "        input_dim=number_of_items,\n",
    "        output_dim=latent_dim_mf,\n",
    "        name=\"mf_item_embedding\",\n",
    "        embeddings_initializer=\"RandomNormal\",\n",
    "        embeddings_regularizer=l2(reg_mf),\n",
    "        input_length=1,\n",
    "    )\n",
    "\n",
    "    mlp_user_embedding = Embedding(\n",
    "        input_dim=number_of_users,\n",
    "        output_dim=latent_dim_mlp,\n",
    "        name=\"mlp_user_embedding\",\n",
    "        embeddings_initializer=\"RandomNormal\",\n",
    "        embeddings_regularizer=l2(reg_mlp),\n",
    "        input_length=1,\n",
    "    )\n",
    "    mlp_item_embedding = Embedding(\n",
    "        input_dim=number_of_items,\n",
    "        output_dim=latent_dim_mlp,\n",
    "        name=\"mlp_item_embedding\",\n",
    "        embeddings_initializer=\"RandomNormal\",\n",
    "        embeddings_regularizer=l2(reg_mlp),\n",
    "        input_length=1,\n",
    "    )\n",
    "\n",
    "    # MF vector\n",
    "    mf_user_latent = Flatten()(mf_user_embedding(user))\n",
    "    mf_item_latent = Flatten()(mf_item_embedding(item))\n",
    "    mf_cat_latent = Multiply()([mf_user_latent, mf_item_latent])\n",
    "\n",
    "    # MLP vector\n",
    "    mlp_user_latent = Flatten()(mlp_user_embedding(user))\n",
    "    mlp_item_latent = Flatten()(mlp_item_embedding(item))\n",
    "    mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "\n",
    "    mlp_vector = mlp_cat_latent\n",
    "\n",
    "    # build dense layers for model\n",
    "    for i in range(len(dense_layers)):\n",
    "        layer = Dense(\n",
    "            dense_layers[i],\n",
    "            activity_regularizer=l2(reg_layers[i]),\n",
    "            activation=activation_dense,\n",
    "            name=\"layer%d\" % i,\n",
    "        )\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    predict_layer = Concatenate()([mf_cat_latent, mlp_vector])\n",
    "\n",
    "    result = Dense(\n",
    "        1, activation=\"sigmoid\", kernel_initializer=\"lecun_uniform\", name=\"interaction\"\n",
    "    )\n",
    "\n",
    "    output = result(predict_layer)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[user, item], outputs=[output],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"neural_collaborative_filtering\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding (Embedding)  (None, 16)           15088       user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mlp_item_embedding (Embedding)  (None, 16)           26912       item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_54 (Flatten)            (None, 16)           0           mlp_user_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_55 (Flatten)            (None, 16)           0           mlp_item_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "mf_user_embedding (Embedding)   (None, 4)            3772        user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mf_item_embedding (Embedding)   (None, 4)            6728        item_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 32)           0           flatten_54[0][0]                 \n",
      "                                                                 flatten_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_52 (Flatten)            (None, 4)            0           mf_user_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_53 (Flatten)            (None, 4)            0           mf_item_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer0 (Dense)                  (None, 8)            264         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 4)            0           flatten_52[0][0]                 \n",
      "                                                                 flatten_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 4)            36          layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8)            0           multiply_13[0][0]                \n",
      "                                                                 layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "interaction (Dense)             (None, 1)            9           concatenate_27[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 52,809\n",
      "Trainable params: 52,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "n_users, n_items = data['train'].shape\n",
    "ncf_model = create_ncf(n_users, n_items)\n",
    "\n",
    "ncf_model.compile(optimizer=Adam(lr=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[\n",
    "                      tf.keras.metrics.TruePositives(name='tp'),\n",
    "                      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "#                       tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "#                       tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "#                       tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "#                       tf.keras.metrics.Precision(name='precision'),\n",
    "#                       tf.keras.metrics.Recall(name='recall'),\n",
    "                      tf.keras.metrics.AUC(name='auc')\n",
    "                  ])\n",
    "ncf_model._name = 'neural_collaborative_filtering'\n",
    "ncf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_dataset(df: pd.DataFrame, targets: List[str], val_split: float = 0.1, batch_size: int = 512, seed = 42):\n",
    "    \"\"\"Make TensorFlow dataset from Pandas DataFrame.\n",
    "    :param df: input DataFrame - only contains features and target(s)\n",
    "    :param targets: list of columns names corresponding to targets\n",
    "    :param val_split: fraction of the data that should be used for validation\n",
    "    :param batch_size: batch size for training\n",
    "    :param seed: random seed for shuffling the data - setting to `None` will not shuffle the data\"\"\"\n",
    "    \n",
    "    n_val = round(df.shape[0]*val_split)\n",
    "    if seed:\n",
    "        x = df.sample(frac=1, random_state=seed).to_dict('series')  # shuffle all the rows\n",
    "    else:\n",
    "        x = df.to_dict('series')\n",
    "    y = dict()\n",
    "    for t in targets:\n",
    "        y[t] = x.pop(t)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    \n",
    "    ds_val = (\n",
    "        ds\n",
    "        .take(n_val)\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "    ds_train = (\n",
    "        ds\n",
    "        .skip(n_val)\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation datasets\n",
    "ds_train, ds_val = make_tf_dataset(df_train, ['interaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/inteligencia-superficial/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2/2789 [..............................] - ETA: 2:56 - loss: 1.6011 - tp: 15.0000 - fp: 299.0000 - auc: 0.4808WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.1239s). Check your callbacks.\n",
      "2789/2789 [==============================] - 8s 3ms/step - loss: 0.1405 - tp: 8101.0000 - fp: 5122.0000 - auc: 0.8893 - val_loss: 0.1202 - val_tp: 1303.0000 - val_fp: 671.0000 - val_auc: 0.9195\n",
      "Epoch 2/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1154 - tp: 14415.0000 - fp: 7976.0000 - auc: 0.9266 - val_loss: 0.1146 - val_tp: 1768.0000 - val_fp: 933.0000 - val_auc: 0.9292\n",
      "Epoch 3/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1112 - tp: 16576.0000 - fp: 9071.0000 - auc: 0.9338 - val_loss: 0.1126 - val_tp: 1926.0000 - val_fp: 1033.0000 - val_auc: 0.9324\n",
      "Epoch 4/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1095 - tp: 17351.0000 - fp: 9495.0000 - auc: 0.9368 - val_loss: 0.1116 - val_tp: 1964.0000 - val_fp: 1065.0000 - val_auc: 0.9339\n",
      "Epoch 5/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1085 - tp: 17741.0000 - fp: 9691.0000 - auc: 0.9385 - val_loss: 0.1109 - val_tp: 1990.0000 - val_fp: 1064.0000 - val_auc: 0.9349\n",
      "Epoch 6/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1078 - tp: 18069.0000 - fp: 9784.0000 - auc: 0.9397 - val_loss: 0.1105 - val_tp: 2002.0000 - val_fp: 1094.0000 - val_auc: 0.9357\n",
      "Epoch 7/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1072 - tp: 18257.0000 - fp: 9852.0000 - auc: 0.9405 - val_loss: 0.1101 - val_tp: 2020.0000 - val_fp: 1099.0000 - val_auc: 0.9362\n",
      "Epoch 8/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1067 - tp: 18381.0000 - fp: 9909.0000 - auc: 0.9412 - val_loss: 0.1098 - val_tp: 2039.0000 - val_fp: 1094.0000 - val_auc: 0.9364\n",
      "Epoch 9/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1063 - tp: 18555.0000 - fp: 9911.0000 - auc: 0.9418 - val_loss: 0.1096 - val_tp: 2076.0000 - val_fp: 1090.0000 - val_auc: 0.9368\n",
      "Epoch 10/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1060 - tp: 18712.0000 - fp: 10004.0000 - auc: 0.9423 - val_loss: 0.1094 - val_tp: 2093.0000 - val_fp: 1091.0000 - val_auc: 0.9371\n",
      "Epoch 11/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1057 - tp: 18854.0000 - fp: 9998.0000 - auc: 0.9427 - val_loss: 0.1092 - val_tp: 2101.0000 - val_fp: 1098.0000 - val_auc: 0.9374\n",
      "Epoch 12/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1054 - tp: 18977.0000 - fp: 10036.0000 - auc: 0.9430 - val_loss: 0.1090 - val_tp: 2113.0000 - val_fp: 1102.0000 - val_auc: 0.9376\n",
      "Epoch 13/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1052 - tp: 19110.0000 - fp: 10098.0000 - auc: 0.9433 - val_loss: 0.1089 - val_tp: 2119.0000 - val_fp: 1118.0000 - val_auc: 0.9379\n",
      "Epoch 14/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1050 - tp: 19216.0000 - fp: 10114.0000 - auc: 0.9435 - val_loss: 0.1088 - val_tp: 2112.0000 - val_fp: 1124.0000 - val_auc: 0.9382\n",
      "Epoch 15/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1048 - tp: 19304.0000 - fp: 10137.0000 - auc: 0.9438 - val_loss: 0.1087 - val_tp: 2131.0000 - val_fp: 1128.0000 - val_auc: 0.9383\n",
      "Epoch 16/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1046 - tp: 19389.0000 - fp: 10160.0000 - auc: 0.9440 - val_loss: 0.1086 - val_tp: 2135.0000 - val_fp: 1133.0000 - val_auc: 0.9384\n",
      "Epoch 17/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1045 - tp: 19453.0000 - fp: 10221.0000 - auc: 0.9443 - val_loss: 0.1086 - val_tp: 2145.0000 - val_fp: 1130.0000 - val_auc: 0.9385\n",
      "Epoch 18/20\n",
      "2789/2789 [==============================] - 7s 3ms/step - loss: 0.1044 - tp: 19517.0000 - fp: 10242.0000 - auc: 0.9444 - val_loss: 0.1085 - val_tp: 2155.0000 - val_fp: 1129.0000 - val_auc: 0.9385\n",
      "Epoch 19/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1043 - tp: 19587.0000 - fp: 10268.0000 - auc: 0.9446 - val_loss: 0.1085 - val_tp: 2171.0000 - val_fp: 1126.0000 - val_auc: 0.9384\n",
      "Epoch 20/20\n",
      "2789/2789 [==============================] - 7s 2ms/step - loss: 0.1042 - tp: 19666.0000 - fp: 10302.0000 - auc: 0.9447 - val_loss: 0.1084 - val_tp: 2170.0000 - val_fp: 1129.0000 - val_auc: 0.9383\n",
      "CPU times: user 4min 18s, sys: 1min 14s, total: 5min 32s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define logs and callbacks\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=0)\n",
    "\n",
    "train_hist = ncf_model.fit(ds_train, validation_data=ds_val, epochs=20, callbacks=[tensorboard_callback, early_stopping_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame(train_hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_test = wide_to_long(data['train'], unique_ratings)\n",
    "df_test = pd.DataFrame(long_test, columns = ['user_id', 'item_id', 'interaction'])\n",
    "ds_test, _ = make_tf_dataset(df_test, ['interaction'], val_split=0, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.72 s, sys: 205 ms, total: 3.92 s\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ncf_predictions = ncf_model.predict(ds_test)\n",
    "df_test['ncf_predictions'] = ncf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>interaction</th>\n",
       "      <th>ncf_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.480493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.753207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.174637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.145832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  interaction  ncf_predictions\n",
       "0        0        7            0         0.480493\n",
       "1        0       10            0         0.753207\n",
       "2        0       19            0         0.174637\n",
       "3        0       20            0         0.074821\n",
       "4        0       26            0         0.145832"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "std = df_test.describe().loc['std', 'ncf_predictions']\n",
    "if std < 0.01:\n",
    "    raise ValueError(\"Model predictions have standard deviation of less than 1e-2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural collaborative filtering predictions\n",
      "[[7.00398564e-01 3.53813112e-01 3.48921120e-01 7.65521049e-01]\n",
      " [1.64715350e-01 2.69353390e-03 2.31661499e-02 3.32486629e-03]\n",
      " [3.44626009e-02 1.21375837e-04 1.87519193e-03 1.86479319e-05]\n",
      " [9.94561911e-02 1.72623992e-03 1.26564503e-03 7.40855932e-04]\n",
      " [5.68827629e-01 2.66197205e-01 4.13460135e-02 3.17071855e-01]\n",
      " [3.41444194e-01 4.38211262e-02 1.19111538e-02 4.49252069e-01]\n",
      " [6.45004809e-01 6.24527216e-01 1.02570385e-01 8.42683196e-01]\n",
      " [6.03474379e-01 1.06243074e-01 1.44939423e-02 1.69369459e-01]\n",
      " [1.39996946e-01 1.65140629e-03 1.47922874e-05 2.34645605e-03]\n",
      " [4.31202054e-01 1.01638705e-01 2.72692144e-02 4.99637365e-01]]\n"
     ]
    }
   ],
   "source": [
    "data['ncf_predictions'] = df_test.pivot(index='user_id', columns='item_id', values='ncf_predictions').values\n",
    "print(\"Neural collaborative filtering predictions\")\n",
    "print(data['ncf_predictions'][:10,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At K = 5, we have a precision of 0.10901 and a recall of 0.06512\n"
     ]
    }
   ],
   "source": [
    "precision_ncf = tf.keras.metrics.Precision(top_k=TOP_K)\n",
    "recall_ncf = tf.keras.metrics.Recall(top_k=TOP_K)\n",
    "\n",
    "precision_ncf.update_state(data['test'], data['ncf_predictions'])\n",
    "recall_ncf.update_state(data['test'], data['ncf_predictions'])\n",
    "print(f\"At K = {TOP_K}, we have a precision of {precision_ncf.result().numpy():.5f} and a recall of {recall_ncf.result().numpy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e4c8e09828d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "At K = 5, we have a precision of 0.10795 and a recall of 0.06449\n",
    "CPU times: user 1.45 s, sys: 286 ms, total: 1.74 s\n",
    "Wall time: 1.34 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At K = 5, we have a precision of 0.10562 and a recall of 0.06309\n",
      "CPU times: user 1.44 s, sys: 259 ms, total: 1.7 s\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LightFM model\n",
    "norm = lambda x: (x - np.min(x))/np.ptp(x)\n",
    "lightfm_model = LightFM(loss='warp')\n",
    "lightfm_model.fit(sparse.coo_matrix(data['train']), epochs=20, num_threads=2)\n",
    "\n",
    "lightfm_predictions = lightfm_model.predict(df_test['user_id'].values, df_test['item_id'].values)\n",
    "df_test['lightfm_predictions'] = lightfm_predictions\n",
    "wide_predictions = df_test.pivot(index='user_id', columns='item_id', values='lightfm_predictions').values\n",
    "data['lightfm_predictions'] = norm(wide_predictions)\n",
    "\n",
    "# compute the metrics\n",
    "precision_lightfm = tf.keras.metrics.Precision(top_k=TOP_K)\n",
    "recall_lightfm = tf.keras.metrics.Recall(top_k=TOP_K)\n",
    "precision_lightfm.update_state(data['test'], data['lightfm_predictions'])\n",
    "recall_lightfm.update_state(data['test'], data['lightfm_predictions'])\n",
    "print(f\"At K = {TOP_K}, we have a precision of {precision_lightfm.result().numpy():.5f} and a recall of {recall_lightfm.result().numpy():.5f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTE - new setup has not finished yet\n",
    "make actual predictions - then use same tf function\n",
    "coverage?\n",
    "personalization?\n",
    "\n",
    "if still fails, then do MF separately and check results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def remove_last_layer(\n",
    "    model: keras.Model, is_trainable: bool = False, layernames_prefix: str = \"\"\n",
    ") -> keras.Model:\n",
    "    \"\"\"Returns a new model, without the last layer.\"\"\"\n",
    "    for layer in model.layers:\n",
    "        layer._name = layernames_prefix + layer.name\n",
    "        layer.trainable = is_trainable\n",
    "    return keras.models.Model(inputs=model.inputs, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
