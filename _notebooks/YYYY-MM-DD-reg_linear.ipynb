{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear\n",
    "> Encontrando a melhor linha\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [machine learning, aprendizado supervisionado]\n",
    "<!-- - image: images/chart-preview.png -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "\n",
    "Regressão linear é um modelo supervisionado de machine learning, que busca encontrar a linha que melhor representa um grupo de pontos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senta que lá vem história...\n",
    "\n",
    "Quando eu morava nos Estados Unidos, estudando engenharia mecânica, eu tive um curso sobre experiências - como as incertezas das medidas vinda dos aparelhos se apresentavam nos resultados. Não muito importante. Mas no experimento, eu e meu parceiro (vamos chamá-lo de Túlio) deveríamos pesar um [béquer](https://pt.wikipedia.org/wiki/B%C3%A9quer) com um líquido várias vezes (com diferentes volumes) e decidir se esse líquido era de água ou não - através do post, vamos supor sim.\n",
    "\n",
    "Eu sabia que independentemente do que fosse, a gente deveria ver uma linha de pontos, já que todos líquidos têm uma densidade constante, e a peso total é proporcional ao volume $peso_{total} = densidade \\cdot volume + peso_{béquer}$. Mas para a minha surpresa não foi bem isso que observamos.\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" /></div> --> - manim: expectativa vs realidade\n",
    "\n",
    "**Porquê?**\n",
    "\n",
    "A balança não é exata, e quando medimos o volume também cometemos erros. Quando levamos tudo me consideração, isso faz diferença dependendo da sensibilidade do nosso experimento. Essas erros aleatórios que variam, normalmente são pequenos e não podemos prever chamamos de [ruído](https://pt.wikipedia.org/wiki/Ru%C3%ADdo). Mas mesmo com esse ruído todo, é possível encontrar os pontos se não tivéssemos ruído nenhum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O setup\n",
    "\n",
    "Estamos tentando aproximar a peso de um béquer com água, sendo que sabemos a peso do béquer {% fn 2 %}. Ou seja:\n",
    "\n",
    "* $P \\leftarrow$ funcao do peso total do liquido no becker\n",
    "* $\\rho \\leftarrow$ densidade do liquido\n",
    "* $P_{becker} \\leftarrow$ peso do becker\n",
    "\n",
    "$$P(V) = \\rho V + P_{becker}$$\n",
    "\n",
    "> Tip: Fique atento às letras aqui e o que elas significam, porque vamos usá-las ao longo do post\n",
    "\n",
    "Isso é o que queremos aproximar, mas também observamos o ruído. Com o ruído, a equação pode ser escrita assim:\n",
    "\n",
    "* $\\epsilon \\leftarrow$ ruído (um pequeno erro e aleatório)\n",
    "\n",
    "$$P(V) = \\rho V + P_{becker} + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abordagem ingênua\n",
    "\n",
    "Quando olhamos, fica claro que os pontos formam uma nuvem ao redor de uma reta. E também não é muito dificil de traçar uma linha na mão. Linha traçada e problema resolvido.\n",
    "\n",
    "A primeira coisa que eu pensei foi: vou no olho! Quão difícil pode ser?\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" width=\"30%\"/></div> --> -manim: draw line\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/3ztiZa4eICWGs/giphy.gif\" width=\"30%\"/></div>\n",
    "\n",
    "Mas era bom demais pra ser verdade - o Túlio resolveu ele traçar uma linha também. Que era muito parecida com a minha, mas ele insistia que a dele era uma aproximação melhor. Como que a gente poderia comparar as linhas? Qual era a melhor? Como eu poderia provar pra ele que a **minha** era melhor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A função do erro\n",
    "\n",
    "Nós estamos tentando aproximar a linha que melhor representa os pontos, certo? E, pra cada volume que medimos nós temos um peso observado e o peso que a nossa linha (nossa função) estimava. A diferença entre os dois nos da o erro pra esse ponto! Se somarmos todos os pontos, temos um erro da nossa linha! Ou seja, quem tiver a menor soma de erros tem a melhor linha. Visualmente fica:\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" /></div> --> - manim: erro vertical\n",
    "\n",
    "Mas também temos que tomar cuidado já que temos erros positivos e negativos - se o nosso ponto está acima ou abaixo da linha. Uma maneira fácil de resolver isso é tirar o quadrado desses erros, já que qualquer número (real) ao quadrado vai gerar um outro número positivo. Também sugeri dividirmos pelo número de pontos, pra poder comparar com as aproximações de outros grupos (que coletaram um número diferente de observações).\n",
    "\n",
    "> Note: Nós não precisaríamos dividir pelo número de pontos já para mim e para o Túlio esse número é igual, e estamos vendo quem tem o **menor** erro entre nós.\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" /></div> --> - manim: mse inteiro\n",
    "\n",
    "Na verdade, não estávamos sendo nem um pouco inovativos. Essa métrica para comparar erros de valores numéricos existe e há bastante tempo. Em estatística, é muito comum ver isso como o [erro quadrático médio](https://pt.wikipedia.org/wiki/Erro_quadr%C3%A1tico_m%C3%A9dio) (ou MSE - \"mean squared error\", em inglês). Esse valor nos dá o quão bom uma função (no nosso caso uma linha) aproxima um grupo de pontos numéricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como melhorar o nosso chute?\n",
    "\n",
    "Dúvida resolvida, e os números claramente mostravam que a minha aproximação tinha um erro menor. Mas depois disso, fiquei pensando: \"se nós temos um erro da nossa função, não daria pra encontrar o mínimo dessa função e assim encontrar a menor linha possível?\"\n",
    "\n",
    "Sim. Podemos! Existem um grupo de algorítmos chamados de [algorítmos de otimização](https://pt.wikipedia.org/wiki/Otimiza%C3%A7%C3%A3o), que buscam fazer exatamente isso - encontrar mínimos (ou máximos) de uma função. Um desses algorítmos é o [gradient descent](https://murilo-cunha.github.io/inteligencia-superficial/machine%20learning/algoritmos%20de%20otimiza%C3%A7%C3%A3o/aprendizado%20supervisionado/2020/04/12/grad_desc.html).\n",
    "\n",
    "Vamos ver como é que fica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A nossa linha\n",
    "\n",
    "Antes de falar como fazemos pra melhorar a nossa aproximação, vamos lembrar da definição da nossa linha:\n",
    "\n",
    "* $P \\leftarrow$ função do peso total do líquido no béquer\n",
    "* $\\rho \\leftarrow$ densidade do líquido\n",
    "* $P_{becker} \\leftarrow$ peso do béquer\n",
    "\n",
    "$$P(V) = \\rho V + P_{béquer}$$\n",
    "\n",
    "E, mais formalmente falando, o que eu e o Túlio estávamos procurando são os melhores valores de $\\rho$ e $P_{becker}$, já que são esses valores que definem a reta. E a função do erro então fica:\n",
    "\n",
    "* $E \\leftarrow$ função do erro da nossa linha baseado nas nossas observações\n",
    "* $V_{obs} \\leftarrow$ peso total observado\n",
    "* $P_{obs} \\leftarrow$ peso total observado\n",
    "* $N \\leftarrow$ número de observações\n",
    "\n",
    "\n",
    "$$E(P_{béquer},\\rho) =  \\frac{1}{N}\\sum_{obs=1}^{N}{(P(V_{obs}) - P_{obs})^2}$$\n",
    "\n",
    "Eu sei que é mais intimidador quando colocamos tudo de uma vez em uma equação. Mas lembra que essa equação está descrevendo nada mais é do que discutimos [acima](##A-função-do-erro). Tome seu tempo pra verificar que faz sentido o que está acontecendo aqui - estamos somando os \"erros verticais\" ao quadrado, e dividindo a soma pelo número total de pontos.\n",
    "\n",
    "Agora, como podemos ajustar os valores de $\\rho$ e $P_{becker}$ pra melhorar nossa aproximação?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando o gradient descent\n",
    "\n",
    "Tudo o que precisamos fazer para aplicar o \"gradient descent\" (descida do gradiente) é definir uma função diferenciável que descreve o **erro** da nossa aproximação.\n",
    "\n",
    "![]({{ site.baseurl }}/assets/manim/videos/grad_desc/480p15/CostSteps.gif \"Passo-a-passo visualizado\")\n",
    "\n",
    "Lembre-se que em gradient descent, nós reduzimos o erro dando um passo na direção oposta do gradiente - ou seja, na direção oposta da derivada em cada dimensão{% fn 1 %}. Como que fica o update de cada parâmetro?\n",
    "\n",
    "Mas antes de mergulharmos nas letrinhas, queria lembrar que essa provavelmente vai ser a parte mais confusa, especialmente se essa é a primeira vez que você está vendo isso. Mas vamos com calma. Vamos definir alguns termos daqui a pouco, mas também vamos explicá-los um por um, e até resolver um exemplo. Até o fim desse post tudo vai ficar mais claro. E fique à vontade para ler, pensar e reler, até que você fique confortável.\n",
    "\n",
    "> Warning: Matemática à frente\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" /></div> --> - manim: transformacoes da equacao\n",
    "\n",
    "Ou então, um passo a passo mais detalhado (que parece mais complicado do que realmente é):\n",
    "\n",
    "As derivadas parciais da nossa função de erro:\n",
    "\n",
    "> Note: Para calcular as derivadas parciais da nossa função de erro nós devemos usar a [regra da cadeia](https://pt.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review).\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial P_{becker}}(P_{becker},\\rho) =  \\frac{2}{N}\\sum_{obs=1}^{N}{((P(V_{obs}) - P_{obs}) \\cdot \\frac{\\partial P}{\\partial P_{becker}}(V_{obs}))}$$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\rho}(P_{becker},\\rho) =  \\frac{2}{N}\\sum_{obs=1}^{N}{((P(V_{obs}) - P_{obs}) \\cdot \\frac{\\partial P}{\\partial \\rho}(V_{obs})})$$\n",
    "\n",
    "As [derivadas parciais](https://pt.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives) da nossa linha (função que estamos tentando aproximar os pontos):\n",
    "\n",
    "> Note: A derivada parcial $\\frac{\\partial f}{\\partial x}(x,y)$ é coeficiente da linha tangente a $f$ se $y$ fosse um número constante.\n",
    "\n",
    "$$\\frac{\\partial P}{\\partial P_{becker}}(V_{obs}) = 1$$\n",
    "\n",
    "$$\\frac{\\partial P}{\\partial \\rho}(V_{obs}) = V_{obs}$$\n",
    "\n",
    "\n",
    "O passo para cada dimensão:\n",
    "\n",
    "* $\\alpha \\leftarrow$ a taxa de aprendizado\n",
    "\n",
    "$$P_{becker} \\leftarrow P_{becker} - \\alpha \\cdot \\frac{\\partial E}{\\partial P_{becker}}(P_{becker},\\rho)$$\n",
    "\n",
    "$$\\rho \\leftarrow P_{becker} - \\alpha \\cdot \\frac{\\partial E}{\\partial\\rho}(P_{becker},\\rho)$$\n",
    "\n",
    "E colocando tudo junto:\n",
    "\n",
    "$$P_{becker} \\leftarrow P_{becker} - \\alpha \\cdot \\frac{2}{N}\\sum_{obs=1}^{N}{(P(V_{obs}) - P_{obs})} \\cdot 1$$\n",
    "\n",
    "$$\\rho \\leftarrow \\rho - \\alpha \\cdot \\frac{2}{N}\\sum_{obs=1}^{N}{((P(V_{obs}) - P_{obs}) \\cdot V_{obs}})$$\n",
    "\n",
    "E é \"só\" isso!\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/Ni4cpi0uUkd6U/giphy.gif\" width=\"30%\"/></div>\n",
    "\n",
    "> Tip: Não se sinta intimidado. Lembre-se que a única coisa que estamos fazendo aqui é reduzir o valor de uma função (função do erro). O porquê o algoritmo funciona, ou a intuição por trás do algoritmo não está no escopo desse post. Mas fique à vontade para ler o post sobre [gradient descent](todo_grad) onde mergulhamos mais a fundo.\n",
    "\n",
    "Visualmente, ficaria mais ou menos assim:\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/Ni4cpi0uUkd6U/giphy.gif\" width=\"30%\"/></div> --> manim: points, noise, bad line, better line, best line! (with update on number of steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um exemplo\n",
    "\n",
    "Eu sei que é meio confuso, então vamos resolver um exercício simples: vamos tentar aproximar uma linha com três pontos.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/HBWbIuHvXI2Eo/giphy.gif\" width=\"30%\"/></div>\n",
    "\n",
    "Nesse exemplo, a linha representaria a **densidade real** da água, enquanto os pontos seriam **experimentos realizados**, mas agora vamos usar os termos $x$ e $y$ para simplificar o problema de um jeito que seria mais fácil de generalizar para outros casos.\n",
    "\n",
    "Vamos fingir que pra linha temos:\n",
    "\n",
    "$$f(x) = y = 2 \\cdot x + 1 $$\n",
    "\n",
    "E para os pontos observados:\n",
    "\n",
    "$$p_1 = (1, 2.5)$$\n",
    "$$p_2 = (2, 5.5)$$\n",
    "$$p_3 = (3, 6.5)$$\n",
    "\n",
    "> Note: estamos procurando a linha que reduz o erro, e essa pode ser (e provavelmente seria) diferente da linha ideal, livre de ruído. Nesse exemplo o nosso erro é de sempre $±0.5$, o que não aconteceria na vida real. Por conta disso, o nosso chute vai se aproximar da linha ideal no nosso exemplo.\n",
    "\n",
    "#### Chute inicial\n",
    "\n",
    "Quando eu estava realizando os experimentos, nós tentamos dar um chute inicial que se aproximasse ao máximo da nossa função de verdade. Na prática não é assim que acontece. Um bom primeiro chute reduz o número de passos que vamos dar. Mas na prática, quando lidamos com problemas mais complexos, não sabemos exatamente o que seria um bom ou mal chute, então escolhemos valores aleatórios para os parâmetros nossa linha ($m$ e $b$) - que também podemos chamar de **pesos** da nossa função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-115b17d830b54fdcab210ae4acafd4e6\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-115b17d830b54fdcab210ae4acafd4e6\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-115b17d830b54fdcab210ae4acafd4e6\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-cb1f7f678e23d76b1eada68351d5e297\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Linha\", \"scale\": {\"domain\": [\"Ideal\", \"Chute\"], \"range\": [\"orange\", \"blue\"]}}, \"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Y\"}}, \"selection\": {\"selector002\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}}, {\"data\": {\"name\": \"data-cab9d0b449025f61cbd552ef89ee2f34\"}, \"mark\": {\"type\": \"rule\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Chute\"}, \"y2\": {\"field\": \"Observado\"}}}, {\"data\": {\"name\": \"data-cab9d0b449025f61cbd552ef89ee2f34\"}, \"mark\": {\"type\": \"circle\", \"color\": \"orange\", \"opacity\": 1, \"size\": 40}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Observado\"}}}, {\"data\": {\"name\": \"data-cab9d0b449025f61cbd552ef89ee2f34\"}, \"mark\": {\"type\": \"circle\", \"color\": \"blue\", \"opacity\": 1, \"size\": 40}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Chute\"}}}], \"title\": \"Observa\\u00e7\\u00f5es\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-cb1f7f678e23d76b1eada68351d5e297\": [{\"Y\": 3, \"X\": 1, \"Linha\": \"Ideal\"}, {\"Y\": 5, \"X\": 2, \"Linha\": \"Ideal\"}, {\"Y\": 7, \"X\": 3, \"Linha\": \"Ideal\"}, {\"Y\": 1, \"X\": 1, \"Linha\": \"Chute\"}, {\"Y\": 0, \"X\": 2, \"Linha\": \"Chute\"}, {\"Y\": -1, \"X\": 3, \"Linha\": \"Chute\"}], \"data-cab9d0b449025f61cbd552ef89ee2f34\": [{\"X\": 1, \"Observado\": 2.5, \"Chute\": 1}, {\"X\": 2, \"Observado\": 5.5, \"Chute\": 0}, {\"X\": 3, \"Observado\": 6.5, \"Chute\": -1}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "def y_linha(x_list: List[float], m: float, b: float) -> List:\n",
    "    \"\"\"Retorna os valores de y dado x seguindo comportamento linear.\"\"\"\n",
    "    return [m*x+b for x in x_list]\n",
    "\n",
    "# lists\n",
    "m_chute = -1\n",
    "b_chute = 2\n",
    "x = [1, 2, 3]\n",
    "\n",
    "y_ideal = [3, 5, 7]\n",
    "y_obs = [2.5, 5.5, 6.5]\n",
    "y_chute = y_linha(x, m_chute, b_chute)\n",
    "\n",
    "# dataframes\n",
    "df_linhas = pd.DataFrame({\n",
    "    'Y': y_ideal + y_chute,\n",
    "    'X': x + x,\n",
    "    'Linha': ['Ideal']*3 + ['Chute']*3\n",
    "})\n",
    "\n",
    "df_pontos = pd.DataFrame({\n",
    "    'X': x,\n",
    "    'Observado': y_obs,\n",
    "    'Chute': y_chute\n",
    "})\n",
    "\n",
    "# plots\n",
    "plt_linhas = alt.Chart(df_linhas).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color=alt.Color('Linha', scale=alt.Scale(domain=['Ideal', 'Chute'], range=['orange', 'blue']))\n",
    ")\n",
    "\n",
    "plt_pontos = alt.Chart(df_pontos).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    ")\n",
    "\n",
    "plt_diff = alt.Chart(df_pontos).encode(\n",
    "    alt.X('X:Q'),\n",
    "    alt.Y('Chute:Q'),\n",
    "    alt.Y2('Ideal:Q')\n",
    ")\n",
    "\n",
    "\n",
    "alt.layer(\n",
    "    plt_linhas.mark_line(),\n",
    "    plt_diff.mark_rule(color='red').encode(alt.X('X'), alt.Y('Chute'), alt.Y2('Observado')),\n",
    "    plt_pontos.mark_circle(color='orange', opacity=1, size=40).encode(x='X', y='Observado'),\n",
    "    plt_pontos.mark_circle(color='blue', opacity=1, size=40).encode(x='X', y='Chute'),\n",
    ").properties(title='Observações').interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O erro foi de 29.583 para 1.159! 👏 \n",
      "\n",
      "m := 1.3333333333333335\n",
      "b := 2.966666666666667\n",
      "MSE = 1.1585185185185194\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mse(y_pred: np.array, y_true: np.array) -> float:\n",
    "    \"\"\"Calcule o erro quadrado médio entre dois vetores.\"\"\"\n",
    "    return (np.square(y_pred - y_true)).mean()\n",
    "\n",
    "def _delta(var: float, y_pred: np.array, y_true: np.array, x_obs: np.array, is_b: bool, alpha: float):\n",
    "    \"\"\"Calcule a diferença dos pesos. Ou seja, retorne a derivada do erro quadrático médio multiplicado pela gradiente.\n",
    "    :param var: valor da variável que estamos querendo fazer o update (valor de m ou b)\n",
    "    :param y_pred: numpy array dos valores previstos, do nosso chute\n",
    "    :param y_true: numpy array dos pontos de y encontrados no experimento\n",
    "    :param is_b: valor booleana que indica se estamos fazendo o update de b ou não\n",
    "    :param alpha: a taxa de aprendizado\n",
    "    :param n: número de pontos/experimentos realizados\"\"\"\n",
    "    \n",
    "    def _media(y_pred: np.array, y_true: np.array, x: np.array, is_b: bool):\n",
    "        \"\"\"Calcule o valor da soma que é parte da derivada.\"\"\"\n",
    "        if is_b:\n",
    "            factor = 1\n",
    "        else:\n",
    "            factor = x\n",
    "        \n",
    "        return ((y_pred - y_true) * factor).mean()\n",
    "    \n",
    "    assert y_pred.shape == y_true.shape, \"Número de previsões e observados diferentes.\"\n",
    "    assert y_pred.ndim == y_true.ndim == 1, \"Y devem ser vetores.\"\n",
    "    \n",
    "    media = _media(y_pred, y_true, x_obs, is_b)\n",
    "    \n",
    "    return alpha * 2 * media\n",
    "\n",
    "# converta as listas para um numpy arrays\n",
    "x = np.array(x)\n",
    "y_obs = np.array(y_obs)\n",
    "y_chute = np.array(y_chute)\n",
    "\n",
    "mse_antes = mse(y_obs, y_chute)\n",
    "\n",
    "m_update = m_chute - _delta(m_chute, y_chute, y_obs, x, False, 0.1)\n",
    "b_update = b_chute - _delta(b_chute, y_chute, y_obs, x, True, 0.1)\n",
    "\n",
    "y_update = y_linha(x, m_update, b_update)\n",
    "\n",
    "mse_depois = mse(y_obs, y_update)\n",
    "\n",
    "if mse_depois >= mse_antes:\n",
    "    \"\"\"Verifique que o erro quadrático médio está diminuindo.\"\"\"\n",
    "    raise ValueError('Os valores não estão melhorando 😞 ...')\n",
    "else:\n",
    "    print(f\"O erro foi de {mse_antes:.3f} para {mse_depois:.3f}! 👏 \\n\")\n",
    "\n",
    "print(f\"m := {m_update}\")\n",
    "print(f\"b := {b_update}\")\n",
    "print(f\"MSE = {mse_depois}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, o para o nosso chute inicial, escolhemos:\n",
    "\n",
    "* $m = -1$\n",
    "* $b = 2$\n",
    "\n",
    "$\\therefore f(x) = y = -1 \\cdot x + 2$\n",
    "\n",
    "As linhas vermelhas mostram o erro, e a gente consegue conferir que o erro quadrático médio é:\n",
    "\n",
    "$$E(m,b) =  \\frac{1}{N}\\sum_{obs=1}^{N}{(f(x_{obs}) - y_{obs})^2}$$\n",
    "$$\\therefore E(2,1) =  \\frac{1}{3}\\sum_{obs=1}^{N}{((-1 \\cdot x_{obs} + 2) - y_{obs})^2}$$\n",
    "$$\\therefore E(2,1) =  \\frac{1}{3}((-1 \\cdot 1 + 2) - 2.5)^2 + ((-1 \\cdot 2 + 2) - 3.5)^2 + ((-1 \\cdot 3 + 2) - 6.5)^2)$$\n",
    "$$\\therefore E(2,1) =  29.58$$\n",
    "\n",
    "Uma maneira de intrepertar esse erro é a diferença média ao quadrado. Poderíamos tirar a raiz quadrada do erro para termos um valor mais interpretável, mas como você vai ver daqui a pouco, vamos tirar a derivada desse valor. Raízes complicam esse processo. Além do mais, independentemente ser ao quadrado ou não, estamos procurando o mínimo desse erro.\n",
    "\n",
    "#### Melhorando o chute\n",
    "\n",
    "Lembrando que os nossos são atualizados de acordo com o que discutimos acima:\n",
    "\n",
    "$$m \\leftarrow m - \\alpha \\cdot \\frac{2}{N}\\sum_{obs=1}^{N}{(f(x_{obs}) - y_{obs})} \\cdot x_{obs}$$\n",
    "\n",
    "$$b \\leftarrow b - \\alpha \\cdot \\frac{2}{N}\\sum_{obs=1}^{N}{(f(x_{obs}) - y_{obs})} \\cdot 1$$\n",
    "\n",
    "Vamos escolher nossa taxa de aprendizado arbitrariamente ($\\alpha = 0.1$). Então os nossos novos pesos ficam:\n",
    "\n",
    "$$m \\leftarrow -1 - 0.1 \\cdot \\frac{2}{3}((-1 \\cdot 1 + 2) - 2.5) + ((-1 \\cdot 2 + 2) - 3.5) + ((-1 \\cdot 3 + 2) - 6.5)) \\cdot -1$$\n",
    "\n",
    "$$b \\leftarrow 2 - 0.1 \\cdot \\frac{2}{3}((-1 \\cdot 1 + 2) - 2.5) + ((-1 \\cdot 2 + 2) - 3.5) + ((-1 \\cdot 3 + 2) - 6.5)) \\cdot 1$$\n",
    "\n",
    "$$\\therefore$$\n",
    "\n",
    "$$m \\leftarrow 1.33$$\n",
    "\n",
    "$$b \\leftarrow 2.97$$\n",
    "\n",
    "<!--  gif--> -- manim\n",
    "\n",
    "E com esses novos valores de $m$ e $b$ o nosso erro vai de **29.58** para **1.16** (fique à vontade pra verificar)!\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/lMVNl6XxTvXgs/giphy.gif\" width=\"30%\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-fac0b695689f443e84ed1bbeba9c012f\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-fac0b695689f443e84ed1bbeba9c012f\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-fac0b695689f443e84ed1bbeba9c012f\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-3b8a00a60b3dcaf633a86e948254f817\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Linha\", \"scale\": {\"domain\": [\"Ideal\", \"Update\", \"Inicial\"], \"range\": [\"orange\", \"blue\", \"red\"]}}, \"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Y\"}}, \"selection\": {\"selector003\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}}, {\"data\": {\"name\": \"data-5ba691d47a02c6fa322a10c75f4319f9\"}, \"mark\": {\"type\": \"rule\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Update\"}, \"y2\": {\"field\": \"Observado\"}}}, {\"data\": {\"name\": \"data-5ba691d47a02c6fa322a10c75f4319f9\"}, \"mark\": {\"type\": \"circle\", \"color\": \"orange\", \"opacity\": 1, \"size\": 40}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Observado\"}}}, {\"data\": {\"name\": \"data-5ba691d47a02c6fa322a10c75f4319f9\"}, \"mark\": {\"type\": \"circle\", \"color\": \"blue\", \"opacity\": 1, \"size\": 40}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Update\"}}}], \"title\": \"Observa\\u00e7\\u00f5es\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-3b8a00a60b3dcaf633a86e948254f817\": [{\"Y\": 3.0, \"X\": 1, \"Linha\": \"Ideal\"}, {\"Y\": 5.0, \"X\": 2, \"Linha\": \"Ideal\"}, {\"Y\": 7.0, \"X\": 3, \"Linha\": \"Ideal\"}, {\"Y\": 4.300000000000001, \"X\": 1, \"Linha\": \"Update\"}, {\"Y\": 5.633333333333334, \"X\": 2, \"Linha\": \"Update\"}, {\"Y\": 6.966666666666667, \"X\": 3, \"Linha\": \"Update\"}, {\"Y\": 1.0, \"X\": 1, \"Linha\": \"Inicial\"}, {\"Y\": 0.0, \"X\": 2, \"Linha\": \"Inicial\"}, {\"Y\": -1.0, \"X\": 3, \"Linha\": \"Inicial\"}], \"data-5ba691d47a02c6fa322a10c75f4319f9\": [{\"X\": 1, \"Observado\": 2.5, \"Update\": 4.300000000000001}, {\"X\": 2, \"Observado\": 5.5, \"Update\": 5.633333333333334}, {\"X\": 3, \"Observado\": 6.5, \"Update\": 6.966666666666667}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# converta os numpy arrays de volta para listas (por conveniencia)\n",
    "y_chute = y_chute.tolist()\n",
    "x = x.tolist()\n",
    "\n",
    "# dataframes\n",
    "df_linhas = pd.DataFrame({\n",
    "    'Y': y_ideal + y_update + y_chute,\n",
    "    'X': x + x + x,\n",
    "    'Linha': ['Ideal']*3 + ['Update']*3 + ['Inicial']*3\n",
    "})\n",
    "\n",
    "df_pontos = pd.DataFrame({\n",
    "    'X': x,\n",
    "    'Observado': y_obs,\n",
    "    'Update': y_update\n",
    "})\n",
    "\n",
    "# plots\n",
    "plt_linhas = alt.Chart(df_linhas).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color=alt.Color('Linha', scale=alt.Scale(domain=['Ideal', 'Update', 'Inicial'], range=['orange', 'blue', 'red']))\n",
    ")\n",
    "\n",
    "plt_pontos = alt.Chart(df_pontos).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    ")\n",
    "\n",
    "plt_diff = alt.Chart(df_pontos).encode(\n",
    "    alt.X('X:Q'),\n",
    "    alt.Y('Update:Q'),\n",
    "    alt.Y2('Ideal:Q')\n",
    ")\n",
    "\n",
    "\n",
    "alt.layer(\n",
    "    plt_linhas.mark_line(),\n",
    "    plt_diff.mark_rule(color='red').encode(alt.X('X'), alt.Y('Update'), alt.Y2('Observado')),\n",
    "    plt_pontos.mark_circle(color='orange', opacity=1, size=40).encode(x='X', y='Observado'),\n",
    "    plt_pontos.mark_circle(color='blue', opacity=1, size=40).encode(x='X', y='Update'),\n",
    ").properties(title='Observações').interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E o resto é só repetindo o mesmo processo! O próximo passo eu deixo para você resolver. Como ficaria os novos pesos $m$ e $b$? Qual seria o erro quadrático médio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O erro foi de 0.222 para 0.222! 👏 \n",
      "\n",
      "m := 1.9918348303106774\n",
      "b := 0.8518946832576453\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "# obs - quando estamos treinando um modelo de regressão linear, estamos executamos o código abaixo várias vezes!\n",
    "# A redução do erro em com as iterações é o que chamamos de \"aprender\"\n",
    "\n",
    "# converta as listas para um numpy arrays\n",
    "x = np.array(x)\n",
    "y_obs = np.array(y_obs)\n",
    "y_update = np.array(y_update)\n",
    "\n",
    "mse_antes = mse(y_obs, y_update)\n",
    "\n",
    "mse_init = mse(y_obs, y_chute)\n",
    "m_update = m_update - _delta(m_update, y_update, y_obs, x, False, 0.1)\n",
    "b_update = b_update - _delta(b_update, y_update, y_obs, x, True, 0.1)\n",
    "\n",
    "y_update = y_linha(x, m_update, b_update)\n",
    "\n",
    "mse_depois = mse(y_obs, y_update)\n",
    "\n",
    "if mse_depois >= mse_antes:\n",
    "    \"\"\"Verifique que o erro quadrático médio está diminuindo.\"\"\"\n",
    "    raise ValueError('Os valores não estão melhorando 😞 ...')\n",
    "else:\n",
    "    print(f\"O erro foi de {mse_antes:.3f} para {mse_depois:.3f}! 👏 \\n\")\n",
    "\n",
    "print(f\"m := {m_update}\")\n",
    "print(f\"b := {b_update}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vetorizando tudo\n",
    "\n",
    "Agora, nós poderíamos colocar tudo em vetores. Se a gente fizer tudo certo, os resultados não mudariam. Mas antes de fazer isso, porque vetorizar tudo? Com certeza é mais trabalhoso reescrever tudo em `Numpy` e repensar em como reescrever suas funções. Vale a pena esse esforço? Além do mais, para multiplicar dois vetores nós ainda temos que computar o mesmo número de operações matemáticas certo?\n",
    "\n",
    "Sim, isso é verdade. Mas ainda assim, vale (e muito) o esforço.\n",
    "* `Numpy` arrays são mais eficientes que listas em `Python`. Muitas das operações são implementadas em [C/C++/Cython](https://en.wikipedia.org/wiki/NumPy#The_ndarray_data_structure), o que aumenta a velocidade de execução, além de usar [ponteiros](https://pt.wikipedia.org/wiki/Ponteiro_(programa%C3%A7%C3%A3o)), etc.\n",
    "* Além disso, uma multiplicação de matrizes, por exemplo, engloba várias operações matemáticas (adição/subtração dos elementos da matriz). Passando várias instruções de uma vez só possibilita o computador de paralelizar as operações ([SIMD](https://pt.wikipedia.org/wiki/SIMD)). Se eu e o Túlio tivéssemos de fazer uma multiplicação de matriz na mão, eu poderia pegar a primeira metade, e ele a segunda. Desse jeito, a gente calcularia na metade do tempo.\n",
    "\n",
    "Espero que tenha te convencido. Em Machine Learning, essas operações se repetem tantas vezes que acabam fazendo muita diferença. Colocando tudo em vetores, o nosso exemplo fica assim:\n",
    "\n",
    "$$\\vec{x_{obs}} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "$$\\vec{y} = b + m \\cdot \\begin{bmatrix} 1 \\\\ 2  \\\\ 3 \\end{bmatrix} \\rightarrow \\vec{y} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} b \\\\ m \\end{bmatrix}$$\n",
    "\n",
    "Ou seja, vamos adicionar uma coluna com $1s$ no nosso vetor $x$ para simplificar as coisas. Vamos também chamar $b$ de $w_0$ e $m$ de $w_1$ (se tivéssemos várias variáveis, o peso $w_n$ multiplicaria a variável $n$). Assim nós também temos os vetor de pesos $\\vec{w}$. Da mesma maneira, todas as nossas observações vão ser contidas em um vetor.\n",
    "\n",
    "> info: O vetor $\\vec{x_{obs}}$ virou a matrix $\\bf{X_{obs}}$, que escrever como os $n$ vetores $\\vec{x_{obs}^{(n)}}$ concatenados. Lembrando também que estamos falando de opeações de [matrizes](https://pt.wikipedia.org/wiki/Produto_de_matrizes), e que $\\odot$ é a notação que eu escolhi para a multiplicação por elementos ([Hamadard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))) de matrizes. Falando de matrizes, os índices das linhas e colunas aqui também vão de $0$ a $N-1$, onde $N$ são o número total de linhas ou colunas. Não é pra complicar, mas em [muitas das linguagens de programação eles denotam as matrizes assim (ao contrário da notação matemática comum)](https://en.wikipedia.org/wiki/Zero-based_numbering) - incluindo `python`.\n",
    "\n",
    "$$\\vec{x_{obs}^{(0)}} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\vec{x_{obs}^{(1)}} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "$$\\bf{X_{obs}} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{bmatrix} = \\begin{bmatrix} \\vec{x_{obs}^{(0)}} & \\vec{x_{obs}^{(1)}} \\end{bmatrix}$$\n",
    "\n",
    "$$\\vec{w} =  \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$$\n",
    "\n",
    "$$\\vec{y_{obs}} = \\begin{bmatrix} 2.5 \\\\ 5.5 \\\\ 6.5 \\end{bmatrix}$$\n",
    "\n",
    "$$\\therefore \\vec{y} = f(\\vec{x}) = \\vec{x} \\vec{w}$$\n",
    "\n",
    "$$w_0 \\leftarrow w_0 - \\alpha \\cdot \\frac{2}{N-1}\\sum_{i=0}^{N-1}{((\\vec{x} \\vec{w} - \\vec{y_{obs}})} \\odot \\vec{x_{obs}^{(0)}})_i$$\n",
    "\n",
    "$$w_1 \\leftarrow w_1 - \\alpha \\cdot \\frac{2}{N-1}\\sum_{i=0}^{N-1}{((\\vec{x} \\vec{w} - \\vec{y_{obs}})} \\odot \\vec{x_{obs}^{(1)}})_i$$\n",
    "\n",
    "E colocando todos os updates em uma equação só:\n",
    "\n",
    "$$\\therefore \\vec{w_n} \\leftarrow \\vec{w_n} - \\alpha \\cdot \\frac{2}{N-1}\\sum_{i=0}^{N-1}{((\\vec{x} \\cdot \\vec{w} - \\vec{y_{obs}})} \\odot \\vec{x_{obs}^{(n)}})_i$$\n",
    "\n",
    "> warning: Se você pesquisar, talvez você ache a última em formas ligeiramente diferentes. Quando isso acontecer, de uma olhada também nas definições dos vetores. É muito possível também que diferentes pessoas definam esses vetores de maneiras diferentes.\n",
    "\n",
    "Eu sei que é muita coisa. Para. Pensa. Toma um café. E vê se faz sentido.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/3o7TKTDn976rzVgky4/giphy.gif\" width=\"30%\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porque MSE?\n",
    "Antes, a gente tinha escolhido o MSE (erro quadrático médio) pra descrever o erro da nossa função. Mas, porque usamos esse erro em particular?\n",
    "Na verdade, essa escolha foi arbitrária. Existem outras funções de erro (também chamadas de função de custo/perda) que poderiam ser aplicadas. Inclusive, dependendo do problema algumas são mais interessantes que outras.\n",
    "\n",
    "Por exemplo, poderiamos somar os valores absolutos dos erros (o chamado [erro médio absoluto](https://pt.qwe.wiki/wiki/Mean_absolute_error)) para definirmos os erros. Desde que essas funções sejam contínuas e que nós possamos definir uma derivada (já que a derivada define o passo em direção ao menor erro), vale tudo.\n",
    "\n",
    "Algumas dessas funções são:\n",
    "* [MSE](https://pt.qwe.wiki/wiki/Mean_squared_error)\n",
    "* [MAE](https://pt.qwe.wiki/wiki/Mean_absolute_error)\n",
    "* [Huber Loss](https://pt.qwe.wiki/wiki/Huber_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nomenclatura\n",
    "As funções de erro são independentes do modelo de ML. Existem diferentes nomes na literatura para modelos em que tentamos encontrar a melhor linha para um grupo de pontos. Para essa implementação mais básica, você pode encontrar, entre outros,\n",
    "\n",
    "* (Simple) linear regression\n",
    "* Least squares regression\n",
    "* Best fit curve\n",
    "\n",
    "Além disso, muitas outras implementações mais sofisticadas tomam essa como base. Essas implementações mais sofisticadas levam em consideração o [sobreajuste ('overfitting', em inglês)](https://pt.wikipedia.org/wiki/Sobreajuste), ou então mais variáveis, ou até para aproximar uma função polinomial!\n",
    "\n",
    "Bastante coisa! E de certa maneira, todas elas se baseiam na idéia de usar a derivada para encontrar o ponto com menor erro de uma função, então vale a pena investir o tempo para realmente entender o que está acontecendo aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes\n",
    "\n",
    "{{ 'Veja o post sobre [gradient descent](todo_grad_desc) para mais detalhes.' | fndetail: 1 }}\n",
    "{{ 'Mais precisamente, deveríamos estar falando de *massa* ao invés de *peso*. Mas no nosse approach isso não faz diferença.' | fndetail: 1 }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
