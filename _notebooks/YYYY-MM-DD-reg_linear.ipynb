{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress√£o Linear\n",
    "> Encontrando a melhor linha\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [machine learning, aprendizado supervisionado]\n",
    "<!-- - image: images/chart-preview.png -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "\n",
    "Regress√£o linear √© um modelo supervisionado de machine learning, que busca encontrar a linha que melhor representa um grupo de pontos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senta que l√° vem hist√≥ria...\n",
    "\n",
    "Quando eu morava nos Estados Unidos, estudando engenharia mec√¢nica, eu tive um curso sobre experi√™ncias - como as incertezas das medidas vinda dos aparelhos se apresentavam nos resultados. N√£o muito importante. Mas no experimento, eu e meu parceiro (vamos cham√°-lo de T√∫lio) dever√≠amos pesar um [b√©quer](https://pt.wikipedia.org/wiki/B%C3%A9quer) com um l√≠quido v√°rias vezes (com diferentes volumes) e decidir se esse l√≠quido era de √°gua ou n√£o - atrav√©s do post, vamos supor sim.\n",
    "\n",
    "Eu sabia que independentemente do que fosse, a gente deveria ver uma linha de pontos, j√° que todos l√≠quidos t√™m uma densidade constante, e a peso total √© proporcional ao volume $peso_{total} = densidade \\cdot volume + peso_{b√©quer}$. Mas para a minha surpresa n√£o foi bem isso que observamos.\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" /></div> --> - manim: expectativa vs realidade\n",
    "\n",
    "**Porqu√™?**\n",
    "\n",
    "A balan√ßa n√£o √© exata, e quando medimos o volume tamb√©m cometemos erros. Quando levamos tudo me considera√ß√£o, isso faz diferen√ßa dependendo da sensibilidade do nosso experimento. Essas erros aleat√≥rios que variam, normalmente s√£o pequenos e n√£o podemos prever chamamos de [ru√≠do](https://pt.wikipedia.org/wiki/Ru%C3%ADdo). Mas mesmo com esse ru√≠do todo, √© poss√≠vel encontrar os pontos se n√£o tiv√©ssemos ru√≠do nenhum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O setup\n",
    "\n",
    "Estamos tentando aproximar a peso de um b√©quer com √°gua, sendo que sabemos a peso do b√©quer {% fn 2 %}. Ou seja:\n",
    "\n",
    "* $P \\leftarrow$ funcao do peso total do liquido no becker\n",
    "* $\\rho \\leftarrow$ densidade do liquido\n",
    "* $P_{becker} \\leftarrow$ peso do becker\n",
    "\n",
    "$$P(V) = \\rho V + P_{becker}$$\n",
    "\n",
    "> Tip: Fique atento √†s letras aqui e o que elas significam, porque vamos us√°-las ao longo do post\n",
    "\n",
    "Isso √© o que queremos aproximar, mas tamb√©m observamos o ru√≠do. Com o ru√≠do, a equa√ß√£o pode ser escrita assim:\n",
    "\n",
    "* $\\epsilon \\leftarrow$ ru√≠do (um pequeno erro e aleat√≥rio)\n",
    "\n",
    "$$P(V) = \\rho V + P_{becker} + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abordagem ing√™nua\n",
    "\n",
    "Quando olhamos, fica claro que os pontos formam uma nuvem ao redor de uma reta. E tamb√©m n√£o √© muito dificil de tra√ßar uma linha na m√£o. Linha tra√ßada e problema resolvido.\n",
    "\n",
    "A primeira coisa que eu pensei foi: vou no olho! Qu√£o dif√≠cil pode ser?\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" width=\"30%\"/></div> --> -manim: draw line\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/3ztiZa4eICWGs/giphy.gif\" width=\"30%\"/></div>\n",
    "\n",
    "Mas era bom demais pra ser verdade - o T√∫lio resolveu ele tra√ßar uma linha tamb√©m. Que era muito parecida com a minha, mas ele insistia que a dele era uma aproxima√ß√£o melhor. Como que a gente poderia comparar as linhas? Qual era a melhor? Como eu poderia provar pra ele que a **minha** era melhor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A fun√ß√£o do erro\n",
    "\n",
    "N√≥s estamos tentando aproximar a linha que melhor representa os pontos, certo? E, pra cada volume que medimos n√≥s temos um peso observado e o peso que a nossa linha (nossa fun√ß√£o) estimava. A diferen√ßa entre os dois nos da o erro pra esse ponto! Se somarmos todos os pontos, temos um erro da nossa linha! Ou seja, quem tiver a menor soma de erros tem a melhor linha. Visualmente fica:\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" /></div> --> - manim: erro vertical\n",
    "\n",
    "Mas tamb√©m temos que tomar cuidado j√° que temos erros positivos e negativos - se o nosso ponto est√° acima ou abaixo da linha. Uma maneira f√°cil de resolver isso √© tirar o quadrado desses erros, j√° que qualquer n√∫mero (real) ao quadrado vai gerar um outro n√∫mero positivo. Tamb√©m sugeri dividirmos pelo n√∫mero de pontos, pra poder comparar com as aproxima√ß√µes de outros grupos (que coletaram um n√∫mero diferente de observa√ß√µes).\n",
    "\n",
    "> Note: N√≥s n√£o precisar√≠amos dividir pelo n√∫mero de pontos j√° para mim e para o T√∫lio esse n√∫mero √© igual, e estamos vendo quem tem o **menor** erro entre n√≥s.\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" /></div> --> - manim: mse inteiro\n",
    "\n",
    "Na verdade, n√£o est√°vamos sendo nem um pouco inovativos. Essa m√©trica para comparar erros de valores num√©ricos existe e h√° bastante tempo. Em estat√≠stica, √© muito comum ver isso como o [erro quadr√°tico m√©dio](https://pt.wikipedia.org/wiki/Erro_quadr%C3%A1tico_m%C3%A9dio) (ou MSE - \"mean squared error\", em ingl√™s). Esse valor nos d√° o qu√£o bom uma fun√ß√£o (no nosso caso uma linha) aproxima um grupo de pontos num√©ricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como melhorar o nosso chute?\n",
    "\n",
    "D√∫vida resolvida, e os n√∫meros claramente mostravam que a minha aproxima√ß√£o tinha um erro menor. Mas depois disso, fiquei pensando: \"se n√≥s temos um erro da nossa fun√ß√£o, n√£o daria pra encontrar o m√≠nimo dessa fun√ß√£o e assim encontrar a menor linha poss√≠vel?\"\n",
    "\n",
    "Sim. Podemos! Existem um grupo de algor√≠tmos chamados de [algor√≠tmos de otimiza√ß√£o](https://pt.wikipedia.org/wiki/Otimiza%C3%A7%C3%A3o), que buscam fazer exatamente isso - encontrar m√≠nimos (ou m√°ximos) de uma fun√ß√£o. Um desses algor√≠tmos √© o [gradient descent](https://murilo-cunha.github.io/inteligencia-superficial/machine%20learning/algoritmos%20de%20otimiza%C3%A7%C3%A3o/aprendizado%20supervisionado/2020/04/12/grad_desc.html).\n",
    "\n",
    "Vamos ver como √© que fica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A nossa linha\n",
    "\n",
    "Antes de falar como fazemos pra melhorar a nossa aproxima√ß√£o, vamos lembrar da defini√ß√£o da nossa linha:\n",
    "\n",
    "* $P \\leftarrow$ fun√ß√£o do peso total do l√≠quido no b√©quer\n",
    "* $\\rho \\leftarrow$ densidade do l√≠quido\n",
    "* $P_{becker} \\leftarrow$ peso do b√©quer\n",
    "\n",
    "$$P(V) = \\rho V + P_{b√©quer}$$\n",
    "\n",
    "E, mais formalmente falando, o que eu e o T√∫lio est√°vamos procurando s√£o os melhores valores de $\\rho$ e $P_{becker}$, j√° que s√£o esses valores que definem a reta. E a fun√ß√£o do erro ent√£o fica:\n",
    "\n",
    "* $E \\leftarrow$ fun√ß√£o do erro da nossa linha baseado nas nossas observa√ß√µes\n",
    "* $V_{obs} \\leftarrow$ peso total observado\n",
    "* $P_{obs} \\leftarrow$ peso total observado\n",
    "* $N \\leftarrow$ n√∫mero de observa√ß√µes\n",
    "\n",
    "\n",
    "$$E(P_{b√©quer},\\rho) =  \\frac{1}{N}\\sum_{obs=1}^{N}{(P(V_{obs}) - P_{obs})^2}$$\n",
    "\n",
    "Eu sei que √© mais intimidador quando colocamos tudo de uma vez em uma equa√ß√£o. Mas lembra que essa equa√ß√£o est√° descrevendo nada mais √© do que discutimos [acima](##A-fun√ß√£o-do-erro). Tome seu tempo pra verificar que faz sentido o que est√° acontecendo aqui - estamos somando os \"erros verticais\" ao quadrado, e dividindo a soma pelo n√∫mero total de pontos.\n",
    "\n",
    "Agora, como podemos ajustar os valores de $\\rho$ e $P_{becker}$ pra melhorar nossa aproxima√ß√£o?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando o gradient descent\n",
    "\n",
    "Tudo o que precisamos fazer para aplicar o \"gradient descent\" (descida do gradiente) √© definir uma fun√ß√£o diferenci√°vel que descreve o **erro** da nossa aproxima√ß√£o.\n",
    "\n",
    "![]({{ site.baseurl }}/assets/manim/videos/grad_desc/480p15/CostSteps.gif \"Passo-a-passo visualizado\")\n",
    "\n",
    "Lembre-se que em gradient descent, n√≥s reduzimos o erro dando um passo na dire√ß√£o oposta do gradiente - ou seja, na dire√ß√£o oposta da derivada em cada dimens√£o{% fn 1 %}. Como que fica o update de cada par√¢metro?\n",
    "\n",
    "Mas antes de mergulharmos nas letrinhas, queria lembrar que essa provavelmente vai ser a parte mais confusa, especialmente se essa √© a primeira vez que voc√™ est√° vendo isso. Mas vamos com calma. Vamos definir alguns termos daqui a pouco, mas tamb√©m vamos explic√°-los um por um, e at√© resolver um exemplo. At√© o fim desse post tudo vai ficar mais claro. E fique √† vontade para ler, pensar e reler, at√© que voc√™ fique confort√°vel.\n",
    "\n",
    "> Warning: Matem√°tica √† frente\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"\" /></div> --> - manim: transformacoes da equacao\n",
    "\n",
    "Ou ent√£o, um passo a passo mais detalhado (que parece mais complicado do que realmente √©):\n",
    "\n",
    "As derivadas parciais da nossa fun√ß√£o de erro:\n",
    "\n",
    "> Note: Para calcular as derivadas parciais da nossa fun√ß√£o de erro n√≥s devemos usar a [regra da cadeia](https://pt.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review).\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial P_{becker}}(P_{becker},\\rho) =  \\frac{2}{N}\\sum_{obs=1}^{N}{((P(V_{obs}) - P_{obs}) \\cdot \\frac{\\partial P}{\\partial P_{becker}}(V_{obs}))}$$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\rho}(P_{becker},\\rho) =  \\frac{2}{N}\\sum_{obs=1}^{N}{((P(V_{obs}) - P_{obs}) \\cdot \\frac{\\partial P}{\\partial \\rho}(V_{obs})})$$\n",
    "\n",
    "As [derivadas parciais](https://pt.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives) da nossa linha (fun√ß√£o que estamos tentando aproximar os pontos):\n",
    "\n",
    "> Note: A derivada parcial $\\frac{\\partial f}{\\partial x}(x,y)$ √© coeficiente da linha tangente a $f$ se $y$ fosse um n√∫mero constante.\n",
    "\n",
    "$$\\frac{\\partial P}{\\partial P_{becker}}(V_{obs}) = 1$$\n",
    "\n",
    "$$\\frac{\\partial P}{\\partial \\rho}(V_{obs}) = V_{obs}$$\n",
    "\n",
    "\n",
    "O passo para cada dimens√£o:\n",
    "\n",
    "* $\\alpha \\leftarrow$ a taxa de aprendizado\n",
    "\n",
    "$$P_{becker} \\leftarrow P_{becker} - \\alpha \\cdot \\frac{\\partial E}{\\partial P_{becker}}(P_{becker},\\rho)$$\n",
    "\n",
    "$$\\rho \\leftarrow P_{becker} - \\alpha \\cdot \\frac{\\partial E}{\\partial\\rho}(P_{becker},\\rho)$$\n",
    "\n",
    "E colocando tudo junto:\n",
    "\n",
    "$$P_{becker} \\leftarrow P_{becker} - \\alpha \\cdot \\frac{2}{N}\\sum_{obs=1}^{N}{(P(V_{obs}) - P_{obs})} \\cdot 1$$\n",
    "\n",
    "$$\\rho \\leftarrow \\rho - \\alpha \\cdot \\frac{2}{N}\\sum_{obs=1}^{N}{((P(V_{obs}) - P_{obs}) \\cdot V_{obs}})$$\n",
    "\n",
    "E √© \"s√≥\" isso!\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/Ni4cpi0uUkd6U/giphy.gif\" width=\"30%\"/></div>\n",
    "\n",
    "> Tip: N√£o se sinta intimidado. Lembre-se que a √∫nica coisa que estamos fazendo aqui √© reduzir o valor de uma fun√ß√£o (fun√ß√£o do erro). O porqu√™ o algoritmo funciona, ou a intui√ß√£o por tr√°s do algoritmo n√£o est√° no escopo desse post. Mas fique √† vontade para ler o post sobre [gradient descent](todo_grad) onde mergulhamos mais a fundo.\n",
    "\n",
    "Visualmente, ficaria mais ou menos assim:\n",
    "\n",
    "<!-- <div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/Ni4cpi0uUkd6U/giphy.gif\" width=\"30%\"/></div> --> manim: points, noise, bad line, better line, best line! (with update on number of steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um exemplo\n",
    "\n",
    "Eu sei que √© meio confuso, ent√£o vamos resolver um exerc√≠cio simples: vamos tentar aproximar uma linha com tr√™s pontos.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/HBWbIuHvXI2Eo/giphy.gif\" width=\"30%\"/></div>\n",
    "\n",
    "Nesse exemplo, a linha representaria a **densidade real** da √°gua, enquanto os pontos seriam **experimentos realizados**, mas agora vamos usar os termos $x$ e $y$ para simplificar o problema de um jeito que seria mais f√°cil de generalizar para outros casos.\n",
    "\n",
    "Vamos fingir que pra linha temos:\n",
    "\n",
    "$$f(x) = y = 2 \\cdot x + 1 $$\n",
    "\n",
    "E para os pontos observados:\n",
    "\n",
    "$$p_1 = (1, 2.5)$$\n",
    "$$p_2 = (2, 5.5)$$\n",
    "$$p_3 = (3, 6.5)$$\n",
    "\n",
    "> Note: estamos procurando a linha que reduz o erro, e essa pode ser (e provavelmente seria) diferente da linha ideal, livre de ru√≠do. Nesse exemplo o nosso erro √© de sempre $¬±0.5$, o que n√£o aconteceria na vida real. Por conta disso, o nosso chute vai se aproximar da linha ideal no nosso exemplo.\n",
    "\n",
    "#### Chute inicial\n",
    "\n",
    "Quando eu estava realizando os experimentos, n√≥s tentamos dar um chute inicial que se aproximasse ao m√°ximo da nossa fun√ß√£o de verdade. Na pr√°tica n√£o √© assim que acontece. Um bom primeiro chute reduz o n√∫mero de passos que vamos dar. Mas na pr√°tica, quando lidamos com problemas mais complexos, n√£o sabemos exatamente o que seria um bom ou mal chute, ent√£o escolhemos valores aleat√≥rios para os par√¢metros nossa linha ($m$ e $b$) - que tamb√©m podemos chamar de **pesos** da nossa fun√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-115b17d830b54fdcab210ae4acafd4e6\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-115b17d830b54fdcab210ae4acafd4e6\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-115b17d830b54fdcab210ae4acafd4e6\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-cb1f7f678e23d76b1eada68351d5e297\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Linha\", \"scale\": {\"domain\": [\"Ideal\", \"Chute\"], \"range\": [\"orange\", \"blue\"]}}, \"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Y\"}}, \"selection\": {\"selector002\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}}, {\"data\": {\"name\": \"data-cab9d0b449025f61cbd552ef89ee2f34\"}, \"mark\": {\"type\": \"rule\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Chute\"}, \"y2\": {\"field\": \"Observado\"}}}, {\"data\": {\"name\": \"data-cab9d0b449025f61cbd552ef89ee2f34\"}, \"mark\": {\"type\": \"circle\", \"color\": \"orange\", \"opacity\": 1, \"size\": 40}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Observado\"}}}, {\"data\": {\"name\": \"data-cab9d0b449025f61cbd552ef89ee2f34\"}, \"mark\": {\"type\": \"circle\", \"color\": \"blue\", \"opacity\": 1, \"size\": 40}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Chute\"}}}], \"title\": \"Observa\\u00e7\\u00f5es\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-cb1f7f678e23d76b1eada68351d5e297\": [{\"Y\": 3, \"X\": 1, \"Linha\": \"Ideal\"}, {\"Y\": 5, \"X\": 2, \"Linha\": \"Ideal\"}, {\"Y\": 7, \"X\": 3, \"Linha\": \"Ideal\"}, {\"Y\": 1, \"X\": 1, \"Linha\": \"Chute\"}, {\"Y\": 0, \"X\": 2, \"Linha\": \"Chute\"}, {\"Y\": -1, \"X\": 3, \"Linha\": \"Chute\"}], \"data-cab9d0b449025f61cbd552ef89ee2f34\": [{\"X\": 1, \"Observado\": 2.5, \"Chute\": 1}, {\"X\": 2, \"Observado\": 5.5, \"Chute\": 0}, {\"X\": 3, \"Observado\": 6.5, \"Chute\": -1}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "def y_linha(x_list: List[float], m: float, b: float) -> List:\n",
    "    \"\"\"Retorna os valores de y dado x seguindo comportamento linear.\"\"\"\n",
    "    return [m*x+b for x in x_list]\n",
    "\n",
    "# lists\n",
    "m_chute = -1\n",
    "b_chute = 2\n",
    "x = [1, 2, 3]\n",
    "\n",
    "y_ideal = [3, 5, 7]\n",
    "y_obs = [2.5, 5.5, 6.5]\n",
    "y_chute = y_linha(x, m_chute, b_chute)\n",
    "\n",
    "# dataframes\n",
    "df_linhas = pd.DataFrame({\n",
    "    'Y': y_ideal + y_chute,\n",
    "    'X': x + x,\n",
    "    'Linha': ['Ideal']*3 + ['Chute']*3\n",
    "})\n",
    "\n",
    "df_pontos = pd.DataFrame({\n",
    "    'X': x,\n",
    "    'Observado': y_obs,\n",
    "    'Chute': y_chute\n",
    "})\n",
    "\n",
    "# plots\n",
    "plt_linhas = alt.Chart(df_linhas).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color=alt.Color('Linha', scale=alt.Scale(domain=['Ideal', 'Chute'], range=['orange', 'blue']))\n",
    ")\n",
    "\n",
    "plt_pontos = alt.Chart(df_pontos).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    ")\n",
    "\n",
    "plt_diff = alt.Chart(df_pontos).encode(\n",
    "    alt.X('X:Q'),\n",
    "    alt.Y('Chute:Q'),\n",
    "    alt.Y2('Ideal:Q')\n",
    ")\n",
    "\n",
    "\n",
    "alt.layer(\n",
    "    plt_linhas.mark_line(),\n",
    "    plt_diff.mark_rule(color='red').encode(alt.X('X'), alt.Y('Chute'), alt.Y2('Observado')),\n",
    "    plt_pontos.mark_circle(color='orange', opacity=1, size=40).encode(x='X', y='Observado'),\n",
    "    plt_pontos.mark_circle(color='blue', opacity=1, size=40).encode(x='X', y='Chute'),\n",
    ").properties(title='Observa√ß√µes').interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O erro foi de 29.583 para 1.159! üëè \n",
      "\n",
      "m := 1.3333333333333335\n",
      "b := 2.966666666666667\n",
      "MSE = 1.1585185185185194\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mse(y_pred: np.array, y_true: np.array) -> float:\n",
    "    \"\"\"Calcule o erro quadrado m√©dio entre dois vetores.\"\"\"\n",
    "    return (np.square(y_pred - y_true)).mean()\n",
    "\n",
    "def _delta(var: float, y_pred: np.array, y_true: np.array, x_obs: np.array, is_b: bool, alpha: float):\n",
    "    \"\"\"Calcule a diferen√ßa dos pesos. Ou seja, retorne a derivada do erro quadr√°tico m√©dio multiplicado pela gradiente.\n",
    "    :param var: valor da vari√°vel que estamos querendo fazer o update (valor de m ou b)\n",
    "    :param y_pred: numpy array dos valores previstos, do nosso chute\n",
    "    :param y_true: numpy array dos pontos de y encontrados no experimento\n",
    "    :param is_b: valor booleana que indica se estamos fazendo o update de b ou n√£o\n",
    "    :param alpha: a taxa de aprendizado\n",
    "    :param n: n√∫mero de pontos/experimentos realizados\"\"\"\n",
    "    \n",
    "    def _media(y_pred: np.array, y_true: np.array, x: np.array, is_b: bool):\n",
    "        \"\"\"Calcule o valor da soma que √© parte da derivada.\"\"\"\n",
    "        if is_b:\n",
    "            factor = 1\n",
    "        else:\n",
    "            factor = x\n",
    "        \n",
    "        return ((y_pred - y_true) * factor).mean()\n",
    "    \n",
    "    assert y_pred.shape == y_true.shape, \"N√∫mero de previs√µes e observados diferentes.\"\n",
    "    assert y_pred.ndim == y_true.ndim == 1, \"Y devem ser vetores.\"\n",
    "    \n",
    "    media = _media(y_pred, y_true, x_obs, is_b)\n",
    "    \n",
    "    return alpha * 2 * media\n",
    "\n",
    "# converta as listas para um numpy arrays\n",
    "x = np.array(x)\n",
    "y_obs = np.array(y_obs)\n",
    "y_chute = np.array(y_chute)\n",
    "\n",
    "mse_antes = mse(y_obs, y_chute)\n",
    "\n",
    "m_update = m_chute - _delta(m_chute, y_chute, y_obs, x, False, 0.1)\n",
    "b_update = b_chute - _delta(b_chute, y_chute, y_obs, x, True, 0.1)\n",
    "\n",
    "y_update = y_linha(x, m_update, b_update)\n",
    "\n",
    "mse_depois = mse(y_obs, y_update)\n",
    "\n",
    "if mse_depois >= mse_antes:\n",
    "    \"\"\"Verifique que o erro quadr√°tico m√©dio est√° diminuindo.\"\"\"\n",
    "    raise ValueError('Os valores n√£o est√£o melhorando üòû ...')\n",
    "else:\n",
    "    print(f\"O erro foi de {mse_antes:.3f} para {mse_depois:.3f}! üëè \\n\")\n",
    "\n",
    "print(f\"m := {m_update}\")\n",
    "print(f\"b := {b_update}\")\n",
    "print(f\"MSE = {mse_depois}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, o para o nosso chute inicial, escolhemos:\n",
    "\n",
    "* $m = -1$\n",
    "* $b = 2$\n",
    "\n",
    "$\\therefore f(x) = y = -1 \\cdot x + 2$\n",
    "\n",
    "As linhas vermelhas mostram o erro, e a gente consegue conferir que o erro quadr√°tico m√©dio √©:\n",
    "\n",
    "$$E(m,b) =  \\frac{1}{N}\\sum_{obs=1}^{N}{(f(x_{obs}) - y_{obs})^2}$$\n",
    "$$\\therefore E(2,1) =  \\frac{1}{3}\\sum_{obs=1}^{N}{((-1 \\cdot x_{obs} + 2) - y_{obs})^2}$$\n",
    "$$\\therefore E(2,1) =  \\frac{1}{3}((-1 \\cdot 1 + 2) - 2.5)^2 + ((-1 \\cdot 2 + 2) - 3.5)^2 + ((-1 \\cdot 3 + 2) - 6.5)^2)$$\n",
    "$$\\therefore E(2,1) =  29.58$$\n",
    "\n",
    "Uma maneira de intrepertar esse erro √© a diferen√ßa m√©dia ao quadrado. Poder√≠amos tirar a raiz quadrada do erro para termos um valor mais interpret√°vel, mas como voc√™ vai ver daqui a pouco, vamos tirar a derivada desse valor. Ra√≠zes complicam esse processo. Al√©m do mais, independentemente ser ao quadrado ou n√£o, estamos procurando o m√≠nimo desse erro.\n",
    "\n",
    "#### Melhorando o chute\n",
    "\n",
    "Lembrando que os nossos s√£o atualizados de acordo com o que discutimos acima:\n",
    "\n",
    "$$m \\leftarrow m - \\alpha \\cdot \\frac{2}{N}\\sum_{obs=1}^{N}{(f(x_{obs}) - y_{obs})} \\cdot x_{obs}$$\n",
    "\n",
    "$$b \\leftarrow b - \\alpha \\cdot \\frac{2}{N}\\sum_{obs=1}^{N}{(f(x_{obs}) - y_{obs})} \\cdot 1$$\n",
    "\n",
    "Vamos escolher nossa taxa de aprendizado arbitrariamente ($\\alpha = 0.1$). Ent√£o os nossos novos pesos ficam:\n",
    "\n",
    "$$m \\leftarrow -1 - 0.1 \\cdot \\frac{2}{3}((-1 \\cdot 1 + 2) - 2.5) + ((-1 \\cdot 2 + 2) - 3.5) + ((-1 \\cdot 3 + 2) - 6.5)) \\cdot -1$$\n",
    "\n",
    "$$b \\leftarrow 2 - 0.1 \\cdot \\frac{2}{3}((-1 \\cdot 1 + 2) - 2.5) + ((-1 \\cdot 2 + 2) - 3.5) + ((-1 \\cdot 3 + 2) - 6.5)) \\cdot 1$$\n",
    "\n",
    "$$\\therefore$$\n",
    "\n",
    "$$m \\leftarrow 1.33$$\n",
    "\n",
    "$$b \\leftarrow 2.97$$\n",
    "\n",
    "<!--  gif--> -- manim\n",
    "\n",
    "E com esses novos valores de $m$ e $b$ o nosso erro vai de **29.58** para **1.16** (fique √† vontade pra verificar)!\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/lMVNl6XxTvXgs/giphy.gif\" width=\"30%\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-fac0b695689f443e84ed1bbeba9c012f\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-fac0b695689f443e84ed1bbeba9c012f\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-fac0b695689f443e84ed1bbeba9c012f\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-3b8a00a60b3dcaf633a86e948254f817\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Linha\", \"scale\": {\"domain\": [\"Ideal\", \"Update\", \"Inicial\"], \"range\": [\"orange\", \"blue\", \"red\"]}}, \"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Y\"}}, \"selection\": {\"selector003\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}}, {\"data\": {\"name\": \"data-5ba691d47a02c6fa322a10c75f4319f9\"}, \"mark\": {\"type\": \"rule\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Update\"}, \"y2\": {\"field\": \"Observado\"}}}, {\"data\": {\"name\": \"data-5ba691d47a02c6fa322a10c75f4319f9\"}, \"mark\": {\"type\": \"circle\", \"color\": \"orange\", \"opacity\": 1, \"size\": 40}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Observado\"}}}, {\"data\": {\"name\": \"data-5ba691d47a02c6fa322a10c75f4319f9\"}, \"mark\": {\"type\": \"circle\", \"color\": \"blue\", \"opacity\": 1, \"size\": 40}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"X\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Update\"}}}], \"title\": \"Observa\\u00e7\\u00f5es\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-3b8a00a60b3dcaf633a86e948254f817\": [{\"Y\": 3.0, \"X\": 1, \"Linha\": \"Ideal\"}, {\"Y\": 5.0, \"X\": 2, \"Linha\": \"Ideal\"}, {\"Y\": 7.0, \"X\": 3, \"Linha\": \"Ideal\"}, {\"Y\": 4.300000000000001, \"X\": 1, \"Linha\": \"Update\"}, {\"Y\": 5.633333333333334, \"X\": 2, \"Linha\": \"Update\"}, {\"Y\": 6.966666666666667, \"X\": 3, \"Linha\": \"Update\"}, {\"Y\": 1.0, \"X\": 1, \"Linha\": \"Inicial\"}, {\"Y\": 0.0, \"X\": 2, \"Linha\": \"Inicial\"}, {\"Y\": -1.0, \"X\": 3, \"Linha\": \"Inicial\"}], \"data-5ba691d47a02c6fa322a10c75f4319f9\": [{\"X\": 1, \"Observado\": 2.5, \"Update\": 4.300000000000001}, {\"X\": 2, \"Observado\": 5.5, \"Update\": 5.633333333333334}, {\"X\": 3, \"Observado\": 6.5, \"Update\": 6.966666666666667}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# converta os numpy arrays de volta para listas (por conveniencia)\n",
    "y_chute = y_chute.tolist()\n",
    "x = x.tolist()\n",
    "\n",
    "# dataframes\n",
    "df_linhas = pd.DataFrame({\n",
    "    'Y': y_ideal + y_update + y_chute,\n",
    "    'X': x + x + x,\n",
    "    'Linha': ['Ideal']*3 + ['Update']*3 + ['Inicial']*3\n",
    "})\n",
    "\n",
    "df_pontos = pd.DataFrame({\n",
    "    'X': x,\n",
    "    'Observado': y_obs,\n",
    "    'Update': y_update\n",
    "})\n",
    "\n",
    "# plots\n",
    "plt_linhas = alt.Chart(df_linhas).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color=alt.Color('Linha', scale=alt.Scale(domain=['Ideal', 'Update', 'Inicial'], range=['orange', 'blue', 'red']))\n",
    ")\n",
    "\n",
    "plt_pontos = alt.Chart(df_pontos).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    ")\n",
    "\n",
    "plt_diff = alt.Chart(df_pontos).encode(\n",
    "    alt.X('X:Q'),\n",
    "    alt.Y('Update:Q'),\n",
    "    alt.Y2('Ideal:Q')\n",
    ")\n",
    "\n",
    "\n",
    "alt.layer(\n",
    "    plt_linhas.mark_line(),\n",
    "    plt_diff.mark_rule(color='red').encode(alt.X('X'), alt.Y('Update'), alt.Y2('Observado')),\n",
    "    plt_pontos.mark_circle(color='orange', opacity=1, size=40).encode(x='X', y='Observado'),\n",
    "    plt_pontos.mark_circle(color='blue', opacity=1, size=40).encode(x='X', y='Update'),\n",
    ").properties(title='Observa√ß√µes').interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E o resto √© s√≥ repetindo o mesmo processo! O pr√≥ximo passo eu deixo para voc√™ resolver. Como ficaria os novos pesos $m$ e $b$? Qual seria o erro quadr√°tico m√©dio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O erro foi de 0.222 para 0.222! üëè \n",
      "\n",
      "m := 1.9918348303106774\n",
      "b := 0.8518946832576453\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "# obs - quando estamos treinando um modelo de regress√£o linear, estamos executamos o c√≥digo abaixo v√°rias vezes!\n",
    "# A redu√ß√£o do erro em com as itera√ß√µes √© o que chamamos de \"aprender\"\n",
    "\n",
    "# converta as listas para um numpy arrays\n",
    "x = np.array(x)\n",
    "y_obs = np.array(y_obs)\n",
    "y_update = np.array(y_update)\n",
    "\n",
    "mse_antes = mse(y_obs, y_update)\n",
    "\n",
    "mse_init = mse(y_obs, y_chute)\n",
    "m_update = m_update - _delta(m_update, y_update, y_obs, x, False, 0.1)\n",
    "b_update = b_update - _delta(b_update, y_update, y_obs, x, True, 0.1)\n",
    "\n",
    "y_update = y_linha(x, m_update, b_update)\n",
    "\n",
    "mse_depois = mse(y_obs, y_update)\n",
    "\n",
    "if mse_depois >= mse_antes:\n",
    "    \"\"\"Verifique que o erro quadr√°tico m√©dio est√° diminuindo.\"\"\"\n",
    "    raise ValueError('Os valores n√£o est√£o melhorando üòû ...')\n",
    "else:\n",
    "    print(f\"O erro foi de {mse_antes:.3f} para {mse_depois:.3f}! üëè \\n\")\n",
    "\n",
    "print(f\"m := {m_update}\")\n",
    "print(f\"b := {b_update}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vetorizando tudo\n",
    "\n",
    "Agora, n√≥s poder√≠amos colocar tudo em vetores. Se a gente fizer tudo certo, os resultados n√£o mudariam. Mas antes de fazer isso, porque vetorizar tudo? Com certeza √© mais trabalhoso reescrever tudo em `Numpy` e repensar em como reescrever suas fun√ß√µes. Vale a pena esse esfor√ßo? Al√©m do mais, para multiplicar dois vetores n√≥s ainda temos que computar o mesmo n√∫mero de opera√ß√µes matem√°ticas certo?\n",
    "\n",
    "Sim, isso √© verdade. Mas ainda assim, vale (e muito) o esfor√ßo.\n",
    "* `Numpy` arrays s√£o mais eficientes que listas em `Python`. Muitas das opera√ß√µes s√£o implementadas em [C/C++/Cython](https://en.wikipedia.org/wiki/NumPy#The_ndarray_data_structure), o que aumenta a velocidade de execu√ß√£o, al√©m de usar [ponteiros](https://pt.wikipedia.org/wiki/Ponteiro_(programa%C3%A7%C3%A3o)), etc.\n",
    "* Al√©m disso, uma multiplica√ß√£o de matrizes, por exemplo, engloba v√°rias opera√ß√µes matem√°ticas (adi√ß√£o/subtra√ß√£o dos elementos da matriz). Passando v√°rias instru√ß√µes de uma vez s√≥ possibilita o computador de paralelizar as opera√ß√µes ([SIMD](https://pt.wikipedia.org/wiki/SIMD)). Se eu e o T√∫lio tiv√©ssemos de fazer uma multiplica√ß√£o de matriz na m√£o, eu poderia pegar a primeira metade, e ele a segunda. Desse jeito, a gente calcularia na metade do tempo.\n",
    "\n",
    "Espero que tenha te convencido. Em Machine Learning, essas opera√ß√µes se repetem tantas vezes que acabam fazendo muita diferen√ßa. Colocando tudo em vetores, o nosso exemplo fica assim:\n",
    "\n",
    "$$\\vec{x_{obs}} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "$$\\vec{y} = b + m \\cdot \\begin{bmatrix} 1 \\\\ 2  \\\\ 3 \\end{bmatrix} \\rightarrow \\vec{y} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} b \\\\ m \\end{bmatrix}$$\n",
    "\n",
    "Ou seja, vamos adicionar uma coluna com $1s$ no nosso vetor $x$ para simplificar as coisas. Vamos tamb√©m chamar $b$ de $w_0$ e $m$ de $w_1$ (se tiv√©ssemos v√°rias vari√°veis, o peso $w_n$ multiplicaria a vari√°vel $n$). Assim n√≥s tamb√©m temos os vetor de pesos $\\vec{w}$. Da mesma maneira, todas as nossas observa√ß√µes v√£o ser contidas em um vetor.\n",
    "\n",
    "> info: O vetor $\\vec{x_{obs}}$ virou a matrix $\\bf{X_{obs}}$, que escrever como os $n$ vetores $\\vec{x_{obs}^{(n)}}$ concatenados. Lembrando tamb√©m que estamos falando de opea√ß√µes de [matrizes](https://pt.wikipedia.org/wiki/Produto_de_matrizes), e que $\\odot$ √© a nota√ß√£o que eu escolhi para a multiplica√ß√£o por elementos ([Hamadard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))) de matrizes. Falando de matrizes, os √≠ndices das linhas e colunas aqui tamb√©m v√£o de $0$ a $N-1$, onde $N$ s√£o o n√∫mero total de linhas ou colunas. N√£o √© pra complicar, mas em [muitas das linguagens de programa√ß√£o eles denotam as matrizes assim (ao contr√°rio da nota√ß√£o matem√°tica comum)](https://en.wikipedia.org/wiki/Zero-based_numbering) - incluindo `python`.\n",
    "\n",
    "$$\\vec{x_{obs}^{(0)}} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\vec{x_{obs}^{(1)}} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "$$\\bf{X_{obs}} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{bmatrix} = \\begin{bmatrix} \\vec{x_{obs}^{(0)}} & \\vec{x_{obs}^{(1)}} \\end{bmatrix}$$\n",
    "\n",
    "$$\\vec{w} =  \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$$\n",
    "\n",
    "$$\\vec{y_{obs}} = \\begin{bmatrix} 2.5 \\\\ 5.5 \\\\ 6.5 \\end{bmatrix}$$\n",
    "\n",
    "$$\\therefore \\vec{y} = f(\\vec{x}) = \\vec{x} \\vec{w}$$\n",
    "\n",
    "$$w_0 \\leftarrow w_0 - \\alpha \\cdot \\frac{2}{N-1}\\sum_{i=0}^{N-1}{((\\vec{x} \\vec{w} - \\vec{y_{obs}})} \\odot \\vec{x_{obs}^{(0)}})_i$$\n",
    "\n",
    "$$w_1 \\leftarrow w_1 - \\alpha \\cdot \\frac{2}{N-1}\\sum_{i=0}^{N-1}{((\\vec{x} \\vec{w} - \\vec{y_{obs}})} \\odot \\vec{x_{obs}^{(1)}})_i$$\n",
    "\n",
    "E colocando todos os updates em uma equa√ß√£o s√≥:\n",
    "\n",
    "$$\\therefore \\vec{w_n} \\leftarrow \\vec{w_n} - \\alpha \\cdot \\frac{2}{N-1}\\sum_{i=0}^{N-1}{((\\vec{x} \\cdot \\vec{w} - \\vec{y_{obs}})} \\odot \\vec{x_{obs}^{(n)}})_i$$\n",
    "\n",
    "> warning: Se voc√™ pesquisar, talvez voc√™ ache a √∫ltima em formas ligeiramente diferentes. Quando isso acontecer, de uma olhada tamb√©m nas defini√ß√µes dos vetores. √â muito poss√≠vel tamb√©m que diferentes pessoas definam esses vetores de maneiras diferentes.\n",
    "\n",
    "Eu sei que √© muita coisa. Para. Pensa. Toma um caf√©. E v√™ se faz sentido.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.giphy.com/media/3o7TKTDn976rzVgky4/giphy.gif\" width=\"30%\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porque MSE?\n",
    "Antes, a gente tinha escolhido o MSE (erro quadr√°tico m√©dio) pra descrever o erro da nossa fun√ß√£o. Mas, porque usamos esse erro em particular?\n",
    "Na verdade, essa escolha foi arbitr√°ria. Existem outras fun√ß√µes de erro (tamb√©m chamadas de fun√ß√£o de custo/perda) que poderiam ser aplicadas. Inclusive, dependendo do problema algumas s√£o mais interessantes que outras.\n",
    "\n",
    "Por exemplo, poderiamos somar os valores absolutos dos erros (o chamado [erro m√©dio absoluto](https://pt.qwe.wiki/wiki/Mean_absolute_error)) para definirmos os erros. Desde que essas fun√ß√µes sejam cont√≠nuas e que n√≥s possamos definir uma derivada (j√° que a derivada define o passo em dire√ß√£o ao menor erro), vale tudo.\n",
    "\n",
    "Algumas dessas fun√ß√µes s√£o:\n",
    "* [MSE](https://pt.qwe.wiki/wiki/Mean_squared_error)\n",
    "* [MAE](https://pt.qwe.wiki/wiki/Mean_absolute_error)\n",
    "* [Huber Loss](https://pt.qwe.wiki/wiki/Huber_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nomenclatura\n",
    "As fun√ß√µes de erro s√£o independentes do modelo de ML. Existem diferentes nomes na literatura para modelos em que tentamos encontrar a melhor linha para um grupo de pontos. Para essa implementa√ß√£o mais b√°sica, voc√™ pode encontrar, entre outros,\n",
    "\n",
    "* (Simple) linear regression\n",
    "* Least squares regression\n",
    "* Best fit curve\n",
    "\n",
    "Al√©m disso, muitas outras implementa√ß√µes mais sofisticadas tomam essa como base. Essas implementa√ß√µes mais sofisticadas levam em considera√ß√£o o [sobreajuste ('overfitting', em ingl√™s)](https://pt.wikipedia.org/wiki/Sobreajuste), ou ent√£o mais vari√°veis, ou at√© para aproximar uma fun√ß√£o polinomial!\n",
    "\n",
    "Bastante coisa! E de certa maneira, todas elas se baseiam na id√©ia de usar a derivada para encontrar o ponto com menor erro de uma fun√ß√£o, ent√£o vale a pena investir o tempo para realmente entender o que est√° acontecendo aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes\n",
    "\n",
    "{{ 'Veja o post sobre [gradient descent](todo_grad_desc) para mais detalhes.' | fndetail: 1 }}\n",
    "{{ 'Mais precisamente, dever√≠amos estar falando de *massa* ao inv√©s de *peso*. Mas no nosse approach isso n√£o faz diferen√ßa.' | fndetail: 1 }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
